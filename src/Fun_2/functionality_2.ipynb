{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1038893\n",
    "\n",
    "fix_random(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "root = \"../../data/ml-25m\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(file: str, nrows=None):\n",
    "    if nrows:\n",
    "        df = pd.read_csv(f\"{root}/{file}\", nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{root}/{file}\")\n",
    "    print(f\"Loaded ml-25m data: {root}/{file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ratings_from_csv(nrows=None):\n",
    "    path = \"ratings.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "\n",
    "    data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "    # todo: drop user id\n",
    "    #data.drop(\"userId\", axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_tag_relevances_from_csv(nrows=None):\n",
    "    path = \"genome-scores.csv\"\n",
    "\n",
    "    return get_data_from_csv(path, nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores_path = \"genome-scores.csv\"\n",
    "ratings_path = \"ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/ratings.csv\n",
      "          userId  movieId  rating\n",
      "0              1      296     5.0\n",
      "1              1      306     3.5\n",
      "2              1      307     5.0\n",
      "3              1      665     5.0\n",
      "4              1      899     3.5\n",
      "...          ...      ...     ...\n",
      "25000090  162541    50872     4.5\n",
      "25000091  162541    55768     2.5\n",
      "25000092  162541    56176     2.0\n",
      "25000093  162541    58559     4.0\n",
      "25000094  162541    63876     5.0\n",
      "\n",
      "[25000095 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ratings = get_ratings_from_csv()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/genome-scores.csv\n",
      "          movieId  tagId  relevance\n",
      "0               1      1    0.02875\n",
      "1               1      2    0.02375\n",
      "2               1      3    0.06250\n",
      "3               1      4    0.07575\n",
      "4               1      5    0.14075\n",
      "...           ...    ...        ...\n",
      "15584443   206499   1124    0.11000\n",
      "15584444   206499   1125    0.04850\n",
      "15584445   206499   1126    0.01325\n",
      "15584446   206499   1127    0.14025\n",
      "15584447   206499   1128    0.03350\n",
      "\n",
      "[15584448 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "genome_scores = get_tag_relevances_from_csv()\n",
    "print(genome_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# films = get_data_from_csv(f\"{root}/{ratings}\")[]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # Fill in missing values with zeros\n",
    "            # X.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnOperation(ratings,X):\n",
    "     # Compute the mean rating for each user\n",
    "     count_rating = ratings.groupby('movieId', as_index=False)['rating'].count()\n",
    "     std= ratings.groupby('movieId', as_index=False)['rating'].std()\n",
    "     std.fillna(0, inplace=True)\n",
    "     min_ratings= ratings.groupby('movieId', as_index=False)['rating'].min()\n",
    "     max_ratings= ratings.groupby('movieId', as_index=False)['rating'].max()\n",
    "     median= ratings.groupby('movieId', as_index=False)['rating'].median()\n",
    "     operation = pd.DataFrame({'movieId':count_rating['movieId'],'count_rating': count_rating['rating'], 'std': std['rating'], 'min': min_ratings['rating'], 'max': max_ratings['rating'], 'median': median['rating']}) \n",
    "     X = pd.merge(X, operation, on='movieId')\n",
    "     X.drop(\"movieId\", axis=1, inplace=True)\n",
    "     return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARE TEST CON AVG, STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ratings, relevance, seed=1038893, batch_size=32, hidden_size1=64, hidden_size2=32, lr=0.001):\n",
    "        # Pivot the relevance DataFrame to create a matrix of tag relevance scores for each movie\n",
    "        relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "\n",
    "        # Compute the mean rating for each user\n",
    "        mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "\n",
    "        # Merge the ratings and relevance data\n",
    "        X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "        X = addColumnOperation(ratings,X)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        ratings = None\n",
    "        #X = X.drop('movieId', axis=1)\n",
    "        y = X['rating']\n",
    "        X = X.drop('rating', axis=1)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        \n",
    "        self.X_test  = self.X_test.to_numpy()\n",
    "        self.X_train = self.X_train.to_numpy()\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(self.X_train)\t\n",
    "        self.X_train = pca.transform(self.X_train)\n",
    "        self.X_test = pca.transform(self.X_test)\n",
    "   \n",
    "            \n",
    "        # Convert the data to PyTorch tensors\n",
    "        self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(self.X_test, dtype=torch.float32)\n",
    "        \n",
    "        self.y_test = np.array(self.y_test)\n",
    "        self.y_train = np.array(self.y_train)\n",
    "        \n",
    "        self.y_test = torch.tensor(self.y_test, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(self.y_train, dtype=torch.float32)\n",
    "        \n",
    "        # Define the neural network architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, 1)\n",
    "        )\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        # Iterate over the entire training dataset multiple times\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            # Set the model to training mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Shuffle the training data\n",
    "            indices = torch.randperm(len(self.X_train))\n",
    "            shuffled_X = self.X_train[indices]\n",
    "            shuffled_y = self.y_train[indices]\n",
    "\n",
    "            # Divide the training data into batches\n",
    "            for i in range(0, len(self.X_train), self.batch_size):\n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Compute the forward pass\n",
    "                #batch_X = shuffled_X[i:i + self.batch_size]\n",
    "                #batch_y = shuffled_y[i:i + self.batch_size]\n",
    "                batch_X = self.X_train\n",
    "                batch_y = self.y_train\n",
    "                \n",
    "                outputs = self.model(batch_X)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                # Compute the backward pass and update the weights\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation data\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.model(self.X_test)\n",
    "                val_loss = self.criterion(val_outputs.squeeze(), self.y_test)\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            # Print the training and validation loss\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} - eta: {elapsed_time:.2f}s -\\tTrain Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    def test(self):\n",
    "    # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Disable gradient calculation to speed up inference\n",
    "        with torch.no_grad():\n",
    "            # Compute the predictions for the test data\n",
    "            y_pred = self.model(self.X_test).squeeze()\n",
    "\n",
    "            # Compute the test loss\n",
    "            test_loss = self.criterion(y_pred, self.y_test)\n",
    "\n",
    "            # Compute the root mean squared error (RMSE) of the test predictions\n",
    "            rmse = torch.sqrt(test_loss)\n",
    "\n",
    "            # Compute the mean squared error (MSE) of the test predictions\n",
    "            mse = test_loss.item()\n",
    "\n",
    "            # Compute the mean absolute error (MAE) of the test predictions\n",
    "            mae = nn.functional.l1_loss(y_pred, self.y_test).item()\n",
    "\n",
    "            # Compute R^2 score of the test predictions\n",
    "            ss_res = torch.sum(torch.square(y_pred - self.y_test))\n",
    "            ss_tot = torch.sum(torch.square(self.y_test - torch.mean(self.y_test)))\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "            # Print the test metrics\n",
    "            print(f'Test RMSE: {rmse.item():.4f}')\n",
    "            print(f'Test MSE: {mse:.4f}')\n",
    "            print(f'Test MAE: {mae:.4f}')\n",
    "            print(f'Test R^2 score: {r2.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(ratings, \n",
    "                        genome_scores,\n",
    "                        seed=1038893, \n",
    "                        batch_size=64, \n",
    "                        hidden_size1=64, \n",
    "                        hidden_size2=64, \n",
    "                        lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - eta: 6.34s -\tTrain Loss: 1.5376, Val Loss: 1.7124\n",
      "Epoch 2/200 - eta: 6.43s -\tTrain Loss: 0.5725, Val Loss: 0.7321\n",
      "Epoch 3/200 - eta: 8.60s -\tTrain Loss: 0.1908, Val Loss: 0.3275\n",
      "Epoch 4/200 - eta: 7.81s -\tTrain Loss: 0.1154, Val Loss: 0.2196\n",
      "Epoch 5/200 - eta: 7.43s -\tTrain Loss: 0.0685, Val Loss: 0.1728\n",
      "Epoch 6/200 - eta: 8.11s -\tTrain Loss: 0.0509, Val Loss: 0.1392\n",
      "Epoch 7/200 - eta: 7.93s -\tTrain Loss: 0.0744, Val Loss: 0.1306\n",
      "Epoch 8/200 - eta: 7.01s -\tTrain Loss: 0.0286, Val Loss: 0.1091\n",
      "Epoch 9/200 - eta: 6.89s -\tTrain Loss: 0.0293, Val Loss: 0.1075\n",
      "Epoch 10/200 - eta: 6.90s -\tTrain Loss: 0.0188, Val Loss: 0.0835\n",
      "Epoch 11/200 - eta: 7.28s -\tTrain Loss: 0.0151, Val Loss: 0.0785\n",
      "Epoch 12/200 - eta: 6.87s -\tTrain Loss: 0.0090, Val Loss: 0.0736\n",
      "Epoch 13/200 - eta: 6.84s -\tTrain Loss: 0.0171, Val Loss: 0.0793\n",
      "Epoch 14/200 - eta: 8.53s -\tTrain Loss: 0.0059, Val Loss: 0.0681\n",
      "Epoch 15/200 - eta: 11.76s -\tTrain Loss: 0.0102, Val Loss: 0.0660\n",
      "Epoch 16/200 - eta: 11.55s -\tTrain Loss: 0.0188, Val Loss: 0.0892\n",
      "Epoch 17/200 - eta: 9.34s -\tTrain Loss: 0.0051, Val Loss: 0.0643\n",
      "Epoch 18/200 - eta: 8.58s -\tTrain Loss: 0.0052, Val Loss: 0.0651\n",
      "Epoch 19/200 - eta: 8.07s -\tTrain Loss: 0.0054, Val Loss: 0.0629\n",
      "Epoch 20/200 - eta: 7.73s -\tTrain Loss: 0.0027, Val Loss: 0.0612\n",
      "Epoch 21/200 - eta: 7.68s -\tTrain Loss: 0.0437, Val Loss: 0.1091\n",
      "Epoch 22/200 - eta: 9.30s -\tTrain Loss: 0.0051, Val Loss: 0.0647\n",
      "Epoch 23/200 - eta: 8.92s -\tTrain Loss: 0.0036, Val Loss: 0.0623\n",
      "Epoch 24/200 - eta: 15.01s -\tTrain Loss: 0.0546, Val Loss: 0.1367\n",
      "Epoch 25/200 - eta: 12.73s -\tTrain Loss: 0.0041, Val Loss: 0.0604\n",
      "Epoch 26/200 - eta: 10.81s -\tTrain Loss: 0.0271, Val Loss: 0.0872\n",
      "Epoch 27/200 - eta: 9.36s -\tTrain Loss: 0.0127, Val Loss: 0.0723\n",
      "Epoch 28/200 - eta: 12.09s -\tTrain Loss: 0.0098, Val Loss: 0.0718\n",
      "Epoch 29/200 - eta: 10.62s -\tTrain Loss: 0.0013, Val Loss: 0.0598\n",
      "Epoch 30/200 - eta: 12.14s -\tTrain Loss: 0.0285, Val Loss: 0.0909\n",
      "Epoch 31/200 - eta: 12.23s -\tTrain Loss: 0.0011, Val Loss: 0.0606\n",
      "Epoch 32/200 - eta: 10.21s -\tTrain Loss: 0.0035, Val Loss: 0.0612\n",
      "Epoch 33/200 - eta: 12.30s -\tTrain Loss: 0.0198, Val Loss: 0.0826\n",
      "Epoch 34/200 - eta: 10.91s -\tTrain Loss: 0.1803, Val Loss: 0.1175\n",
      "Epoch 35/200 - eta: 13.39s -\tTrain Loss: 0.0106, Val Loss: 0.0773\n",
      "Epoch 36/200 - eta: 14.19s -\tTrain Loss: 0.0062, Val Loss: 0.0718\n",
      "Epoch 37/200 - eta: 10.60s -\tTrain Loss: 0.0127, Val Loss: 0.0749\n",
      "Epoch 38/200 - eta: 10.81s -\tTrain Loss: 0.0051, Val Loss: 0.0688\n",
      "Epoch 39/200 - eta: 10.34s -\tTrain Loss: 0.0034, Val Loss: 0.0668\n",
      "Epoch 40/200 - eta: 9.13s -\tTrain Loss: 0.0272, Val Loss: 0.1015\n",
      "Epoch 41/200 - eta: 12.62s -\tTrain Loss: 0.0048, Val Loss: 0.0730\n",
      "Epoch 42/200 - eta: 10.69s -\tTrain Loss: 0.0194, Val Loss: 0.0837\n",
      "Epoch 43/200 - eta: 9.83s -\tTrain Loss: 0.0508, Val Loss: 0.1146\n",
      "Epoch 44/200 - eta: 9.98s -\tTrain Loss: 0.0461, Val Loss: 0.1264\n",
      "Epoch 45/200 - eta: 11.02s -\tTrain Loss: 0.0314, Val Loss: 0.1026\n",
      "Epoch 46/200 - eta: 8.27s -\tTrain Loss: 0.0180, Val Loss: 0.0766\n",
      "Epoch 47/200 - eta: 11.19s -\tTrain Loss: 0.0223, Val Loss: 0.0890\n",
      "Epoch 48/200 - eta: 12.92s -\tTrain Loss: 0.0019, Val Loss: 0.0636\n",
      "Epoch 49/200 - eta: 12.58s -\tTrain Loss: 0.0082, Val Loss: 0.0731\n",
      "Epoch 50/200 - eta: 14.13s -\tTrain Loss: 0.0010, Val Loss: 0.0629\n",
      "Epoch 51/200 - eta: 17.70s -\tTrain Loss: 0.0011, Val Loss: 0.0627\n",
      "Epoch 52/200 - eta: 11.77s -\tTrain Loss: 0.0034, Val Loss: 0.0652\n",
      "Epoch 53/200 - eta: 9.89s -\tTrain Loss: 0.0162, Val Loss: 0.0831\n",
      "Epoch 54/200 - eta: 8.71s -\tTrain Loss: 0.9299, Val Loss: 1.0579\n",
      "Epoch 55/200 - eta: 10.33s -\tTrain Loss: 0.0116, Val Loss: 0.0677\n",
      "Epoch 56/200 - eta: 9.82s -\tTrain Loss: 0.0072, Val Loss: 0.0648\n",
      "Epoch 57/200 - eta: 10.26s -\tTrain Loss: 0.0045, Val Loss: 0.0631\n",
      "Epoch 58/200 - eta: 9.69s -\tTrain Loss: 0.0073, Val Loss: 0.0655\n",
      "Epoch 59/200 - eta: 9.82s -\tTrain Loss: 0.0034, Val Loss: 0.0628\n",
      "Epoch 60/200 - eta: 9.84s -\tTrain Loss: 0.0034, Val Loss: 0.0624\n",
      "Epoch 61/200 - eta: 9.60s -\tTrain Loss: 0.0024, Val Loss: 0.0602\n",
      "Epoch 62/200 - eta: 9.71s -\tTrain Loss: 0.0019, Val Loss: 0.0600\n",
      "Epoch 63/200 - eta: 9.31s -\tTrain Loss: 0.0018, Val Loss: 0.0599\n",
      "Epoch 64/200 - eta: 9.46s -\tTrain Loss: 0.0013, Val Loss: 0.0590\n",
      "Epoch 65/200 - eta: 9.26s -\tTrain Loss: 0.0030, Val Loss: 0.0634\n",
      "Epoch 66/200 - eta: 8.73s -\tTrain Loss: 0.0011, Val Loss: 0.0583\n",
      "Epoch 67/200 - eta: 8.56s -\tTrain Loss: 0.0030, Val Loss: 0.0593\n",
      "Epoch 68/200 - eta: 10.49s -\tTrain Loss: 0.0106, Val Loss: 0.0695\n",
      "Epoch 69/200 - eta: 12.24s -\tTrain Loss: 0.0133, Val Loss: 0.0695\n",
      "Epoch 70/200 - eta: 12.87s -\tTrain Loss: 0.0325, Val Loss: 0.1032\n",
      "Epoch 71/200 - eta: 11.43s -\tTrain Loss: 0.0024, Val Loss: 0.0589\n",
      "Epoch 72/200 - eta: 7.93s -\tTrain Loss: 0.0008, Val Loss: 0.0580\n",
      "Epoch 73/200 - eta: 7.42s -\tTrain Loss: 0.0283, Val Loss: 0.0647\n",
      "Epoch 74/200 - eta: 10.44s -\tTrain Loss: 0.0103, Val Loss: 0.0480\n",
      "Epoch 75/200 - eta: 10.03s -\tTrain Loss: 0.0063, Val Loss: 0.0453\n",
      "Epoch 76/200 - eta: 10.82s -\tTrain Loss: 0.0045, Val Loss: 0.0433\n",
      "Epoch 77/200 - eta: 10.10s -\tTrain Loss: 0.0036, Val Loss: 0.0420\n",
      "Epoch 78/200 - eta: 9.51s -\tTrain Loss: 0.0030, Val Loss: 0.0412\n",
      "Epoch 79/200 - eta: 8.82s -\tTrain Loss: 0.0041, Val Loss: 0.0409\n",
      "Epoch 80/200 - eta: 8.62s -\tTrain Loss: 0.0047, Val Loss: 0.0452\n",
      "Epoch 81/200 - eta: 8.69s -\tTrain Loss: 0.0027, Val Loss: 0.0403\n",
      "Epoch 82/200 - eta: 8.76s -\tTrain Loss: 0.0032, Val Loss: 0.0426\n",
      "Epoch 83/200 - eta: 11.46s -\tTrain Loss: 0.0023, Val Loss: 0.0417\n",
      "Epoch 84/200 - eta: 11.85s -\tTrain Loss: 0.0039, Val Loss: 0.0416\n",
      "Epoch 85/200 - eta: 11.96s -\tTrain Loss: 0.0088, Val Loss: 0.0492\n",
      "Epoch 86/200 - eta: 8.93s -\tTrain Loss: 0.0026, Val Loss: 0.0395\n",
      "Epoch 87/200 - eta: 7.58s -\tTrain Loss: 0.0155, Val Loss: 0.0555\n",
      "Epoch 88/200 - eta: 8.82s -\tTrain Loss: 0.0014, Val Loss: 0.0400\n",
      "Epoch 89/200 - eta: 7.42s -\tTrain Loss: 0.0012, Val Loss: 0.0396\n",
      "Epoch 90/200 - eta: 7.28s -\tTrain Loss: 0.0017, Val Loss: 0.0413\n",
      "Epoch 91/200 - eta: 7.36s -\tTrain Loss: 0.0061, Val Loss: 0.0473\n",
      "Epoch 92/200 - eta: 7.41s -\tTrain Loss: 0.0013, Val Loss: 0.0394\n",
      "Epoch 93/200 - eta: 7.38s -\tTrain Loss: 0.0025, Val Loss: 0.0425\n",
      "Epoch 94/200 - eta: 7.35s -\tTrain Loss: 0.0036, Val Loss: 0.0415\n",
      "Epoch 95/200 - eta: 7.46s -\tTrain Loss: 0.0160, Val Loss: 0.0456\n",
      "Epoch 96/200 - eta: 7.56s -\tTrain Loss: 0.0040, Val Loss: 0.0421\n",
      "Epoch 97/200 - eta: 10.36s -\tTrain Loss: 0.0024, Val Loss: 0.0422\n",
      "Epoch 98/200 - eta: 8.11s -\tTrain Loss: 0.0101, Val Loss: 0.0487\n",
      "Epoch 99/200 - eta: 13.76s -\tTrain Loss: 0.0005, Val Loss: 0.0405\n",
      "Epoch 100/200 - eta: 17.93s -\tTrain Loss: 0.0058, Val Loss: 0.0472\n",
      "Epoch 101/200 - eta: 14.79s -\tTrain Loss: 0.0027, Val Loss: 0.0429\n",
      "Epoch 102/200 - eta: 14.59s -\tTrain Loss: 0.0545, Val Loss: 0.0595\n",
      "Epoch 103/200 - eta: 9.14s -\tTrain Loss: 0.0043, Val Loss: 0.0424\n",
      "Epoch 104/200 - eta: 8.24s -\tTrain Loss: 0.0051, Val Loss: 0.0424\n",
      "Epoch 105/200 - eta: 11.67s -\tTrain Loss: 0.0016, Val Loss: 0.0408\n",
      "Epoch 106/200 - eta: 9.19s -\tTrain Loss: 0.0016, Val Loss: 0.0406\n",
      "Epoch 107/200 - eta: 9.81s -\tTrain Loss: 0.0019, Val Loss: 0.0436\n",
      "Epoch 108/200 - eta: 7.89s -\tTrain Loss: 0.0023, Val Loss: 0.0410\n",
      "Epoch 109/200 - eta: 8.80s -\tTrain Loss: 0.0008, Val Loss: 0.0401\n",
      "Epoch 110/200 - eta: 9.38s -\tTrain Loss: 0.0022, Val Loss: 0.0407\n",
      "Epoch 111/200 - eta: 9.52s -\tTrain Loss: 0.0072, Val Loss: 0.0470\n",
      "Epoch 112/200 - eta: 9.35s -\tTrain Loss: 0.0009, Val Loss: 0.0399\n",
      "Epoch 113/200 - eta: 9.15s -\tTrain Loss: 0.0032, Val Loss: 0.0402\n",
      "Epoch 114/200 - eta: 12.19s -\tTrain Loss: 0.0049, Val Loss: 0.0427\n",
      "Epoch 115/200 - eta: 9.39s -\tTrain Loss: 0.0010, Val Loss: 0.0398\n",
      "Epoch 116/200 - eta: 7.03s -\tTrain Loss: 0.0022, Val Loss: 0.0412\n",
      "Epoch 117/200 - eta: 7.70s -\tTrain Loss: 0.0130, Val Loss: 0.0579\n",
      "Epoch 118/200 - eta: 8.68s -\tTrain Loss: 0.0003, Val Loss: 0.0394\n",
      "Epoch 119/200 - eta: 9.53s -\tTrain Loss: 0.0007, Val Loss: 0.0408\n",
      "Epoch 120/200 - eta: 9.08s -\tTrain Loss: 0.0009, Val Loss: 0.0393\n",
      "Epoch 121/200 - eta: 11.16s -\tTrain Loss: 0.0005, Val Loss: 0.0416\n",
      "Epoch 122/200 - eta: 9.85s -\tTrain Loss: 0.0011, Val Loss: 0.0400\n",
      "Epoch 123/200 - eta: 9.08s -\tTrain Loss: 0.0102, Val Loss: 0.0517\n",
      "Epoch 124/200 - eta: 8.84s -\tTrain Loss: 0.0002, Val Loss: 0.0401\n",
      "Epoch 125/200 - eta: 8.93s -\tTrain Loss: 0.0075, Val Loss: 0.0378\n",
      "Epoch 126/200 - eta: 10.24s -\tTrain Loss: 0.0030, Val Loss: 0.0367\n",
      "Epoch 127/200 - eta: 8.41s -\tTrain Loss: 0.0020, Val Loss: 0.0353\n",
      "Epoch 128/200 - eta: 8.04s -\tTrain Loss: 0.0032, Val Loss: 0.0378\n",
      "Epoch 129/200 - eta: 8.06s -\tTrain Loss: 0.0015, Val Loss: 0.0364\n",
      "Epoch 130/200 - eta: 8.21s -\tTrain Loss: 0.0008, Val Loss: 0.0357\n",
      "Epoch 131/200 - eta: 9.45s -\tTrain Loss: 0.0007, Val Loss: 0.0354\n",
      "Epoch 132/200 - eta: 10.88s -\tTrain Loss: 0.0013, Val Loss: 0.0359\n",
      "Epoch 133/200 - eta: 8.43s -\tTrain Loss: 0.0013, Val Loss: 0.0373\n",
      "Epoch 134/200 - eta: 9.86s -\tTrain Loss: 0.0004, Val Loss: 0.0361\n",
      "Epoch 135/200 - eta: 18.54s -\tTrain Loss: 0.0004, Val Loss: 0.0358\n",
      "Epoch 136/200 - eta: 8.81s -\tTrain Loss: 0.0018, Val Loss: 0.0365\n",
      "Epoch 137/200 - eta: 10.14s -\tTrain Loss: 0.0036, Val Loss: 0.0417\n",
      "Epoch 138/200 - eta: 8.65s -\tTrain Loss: 0.2116, Val Loss: 0.1847\n",
      "Epoch 139/200 - eta: 8.09s -\tTrain Loss: 0.0051, Val Loss: 0.0411\n",
      "Epoch 140/200 - eta: 8.43s -\tTrain Loss: 0.0047, Val Loss: 0.0397\n",
      "Epoch 141/200 - eta: 8.64s -\tTrain Loss: 0.0029, Val Loss: 0.0378\n",
      "Epoch 142/200 - eta: 8.29s -\tTrain Loss: 0.0044, Val Loss: 0.0397\n",
      "Epoch 143/200 - eta: 9.20s -\tTrain Loss: 0.0025, Val Loss: 0.0374\n",
      "Epoch 144/200 - eta: 8.40s -\tTrain Loss: 0.0023, Val Loss: 0.0380\n",
      "Epoch 145/200 - eta: 8.34s -\tTrain Loss: 0.0011, Val Loss: 0.0353\n",
      "Epoch 146/200 - eta: 8.17s -\tTrain Loss: 0.0021, Val Loss: 0.0383\n",
      "Epoch 147/200 - eta: 8.07s -\tTrain Loss: 0.0012, Val Loss: 0.0368\n",
      "Epoch 148/200 - eta: 8.18s -\tTrain Loss: 0.0010, Val Loss: 0.0361\n",
      "Epoch 149/200 - eta: 8.28s -\tTrain Loss: 0.0008, Val Loss: 0.0359\n",
      "Epoch 150/200 - eta: 8.20s -\tTrain Loss: 0.0057, Val Loss: 0.0386\n",
      "Epoch 151/200 - eta: 8.09s -\tTrain Loss: 0.0036, Val Loss: 0.0375\n",
      "Epoch 152/200 - eta: 8.75s -\tTrain Loss: 0.0293, Val Loss: 0.0502\n",
      "Epoch 153/200 - eta: 8.39s -\tTrain Loss: 0.0031, Val Loss: 0.0369\n",
      "Epoch 154/200 - eta: 8.50s -\tTrain Loss: 0.0016, Val Loss: 0.0380\n",
      "Epoch 155/200 - eta: 10.33s -\tTrain Loss: 0.0054, Val Loss: 0.0371\n",
      "Epoch 156/200 - eta: 9.30s -\tTrain Loss: 0.0015, Val Loss: 0.0358\n",
      "Epoch 157/200 - eta: 9.24s -\tTrain Loss: 0.0010, Val Loss: 0.0357\n",
      "Epoch 158/200 - eta: 10.27s -\tTrain Loss: 0.0011, Val Loss: 0.0354\n",
      "Epoch 159/200 - eta: 10.20s -\tTrain Loss: 0.0037, Val Loss: 0.0378\n",
      "Epoch 160/200 - eta: 15.72s -\tTrain Loss: 0.0017, Val Loss: 0.0371\n",
      "Epoch 161/200 - eta: 11.80s -\tTrain Loss: 0.0027, Val Loss: 0.0365\n",
      "Epoch 162/200 - eta: 13.27s -\tTrain Loss: 0.0061, Val Loss: 0.0403\n",
      "Epoch 163/200 - eta: 14.41s -\tTrain Loss: 0.0006, Val Loss: 0.0337\n",
      "Epoch 164/200 - eta: 9.94s -\tTrain Loss: 0.0007, Val Loss: 0.0331\n",
      "Epoch 165/200 - eta: 7.88s -\tTrain Loss: 0.0004, Val Loss: 0.0328\n",
      "Epoch 166/200 - eta: 8.61s -\tTrain Loss: 0.0020, Val Loss: 0.0367\n",
      "Epoch 167/200 - eta: 8.72s -\tTrain Loss: 0.0017, Val Loss: 0.0351\n",
      "Epoch 168/200 - eta: 8.01s -\tTrain Loss: 0.0009, Val Loss: 0.0330\n",
      "Epoch 169/200 - eta: 8.18s -\tTrain Loss: 0.0035, Val Loss: 0.0352\n",
      "Epoch 170/200 - eta: 8.96s -\tTrain Loss: 0.0005, Val Loss: 0.0330\n",
      "Epoch 171/200 - eta: 8.92s -\tTrain Loss: 0.0029, Val Loss: 0.0337\n",
      "Epoch 172/200 - eta: 8.10s -\tTrain Loss: 0.0053, Val Loss: 0.0284\n",
      "Epoch 173/200 - eta: 8.01s -\tTrain Loss: 0.0024, Val Loss: 0.0259\n",
      "Epoch 174/200 - eta: 8.38s -\tTrain Loss: 0.0017, Val Loss: 0.0253\n",
      "Epoch 175/200 - eta: 8.09s -\tTrain Loss: 0.0013, Val Loss: 0.0249\n",
      "Epoch 176/200 - eta: 10.86s -\tTrain Loss: 0.0011, Val Loss: 0.0246\n",
      "Epoch 177/200 - eta: 10.80s -\tTrain Loss: 0.0009, Val Loss: 0.0244\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 50087664 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mtrain(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[67], line 66\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# Shuffle the training data\u001b[39;00m\n\u001b[0;32m     65\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train))\n\u001b[1;32m---> 66\u001b[0m shuffled_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train[indices]\n\u001b[0;32m     67\u001b[0m shuffled_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train[indices]\n\u001b[0;32m     69\u001b[0m \u001b[39m# Divide the training data into batches\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 50087664 bytes."
     ]
    }
   ],
   "source": [
    "model.train(num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.0853\n",
      "Test MSE: 0.0073\n",
      "Test MAE: 0.0637\n",
      "Test R^2 score: 0.9684\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87bd6c09cddab90a09f19a55e9245e3497687050bc7d26cfabaa798d009918f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
