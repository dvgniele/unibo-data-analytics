{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1038893\n",
    "\n",
    "fix_random(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "root = \"../../data/ml-25m\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(file: str, nrows=None):\n",
    "    if nrows:\n",
    "        df = pd.read_csv(f\"{root}/{file}\", nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{root}/{file}\")\n",
    "    print(f\"Loaded ml-25m data: {root}/{file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ratings_from_csv(nrows=None):\n",
    "    path = \"ratings.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "\n",
    "    data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "    # todo: drop user id\n",
    "    #data.drop(\"userId\", axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_tag_relevances_from_csv(nrows=None):\n",
    "    path = \"genome-scores.csv\"\n",
    "\n",
    "    return get_data_from_csv(path, nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores_path = \"genome-scores.csv\"\n",
    "ratings_path = \"ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/ratings.csv\n",
      "          userId  movieId  rating\n",
      "0              1      296     5.0\n",
      "1              1      306     3.5\n",
      "2              1      307     5.0\n",
      "3              1      665     5.0\n",
      "4              1      899     3.5\n",
      "...          ...      ...     ...\n",
      "25000090  162541    50872     4.5\n",
      "25000091  162541    55768     2.5\n",
      "25000092  162541    56176     2.0\n",
      "25000093  162541    58559     4.0\n",
      "25000094  162541    63876     5.0\n",
      "\n",
      "[25000095 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ratings = get_ratings_from_csv()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/genome-scores.csv\n",
      "          movieId  tagId  relevance\n",
      "0               1      1    0.02875\n",
      "1               1      2    0.02375\n",
      "2               1      3    0.06250\n",
      "3               1      4    0.07575\n",
      "4               1      5    0.14075\n",
      "...           ...    ...        ...\n",
      "15584443   206499   1124    0.11000\n",
      "15584444   206499   1125    0.04850\n",
      "15584445   206499   1126    0.01325\n",
      "15584446   206499   1127    0.14025\n",
      "15584447   206499   1128    0.03350\n",
      "\n",
      "[15584448 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "genome_scores = get_tag_relevances_from_csv()\n",
    "print(genome_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# films = get_data_from_csv(f\"{root}/{ratings}\")[]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # Fill in missing values with zeros\n",
    "            # X.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnOperation(ratings,X):\n",
    "     # Compute the mean rating for each user\n",
    "     count_rating = ratings.groupby('movieId', as_index=False)['rating'].count()\n",
    "     std= ratings.groupby('movieId', as_index=False)['rating'].std()\n",
    "     std.fillna(0, inplace=True)\n",
    "     min_ratings= ratings.groupby('movieId', as_index=False)['rating'].min()\n",
    "     max_ratings= ratings.groupby('movieId', as_index=False)['rating'].max()\n",
    "     median= ratings.groupby('movieId', as_index=False)['rating'].median()\n",
    "     operation = pd.DataFrame({'movieId':count_rating['movieId'],'count_rating': count_rating['rating'], 'std': std['rating'], 'min': min_ratings['rating'], 'max': max_ratings['rating'], 'median': median['rating']}) \n",
    "     X = pd.merge(X, operation, on='movieId')\n",
    "     X.drop(\"movieId\", axis=1, inplace=True)\n",
    "     return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARE TEST CON AVG, STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ratings, relevance, seed=1038893, batch_size=32, hidden_size1=64, hidden_size2=32, lr=0.001):\n",
    "        # Pivot the relevance DataFrame to create a matrix of tag relevance scores for each movie\n",
    "        relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "\n",
    "        # Compute the mean rating for each user\n",
    "        mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "\n",
    "        # Merge the ratings and relevance data\n",
    "        X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "        #X = addColumnOperation(ratings,X)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        ratings = None\n",
    "        #X = X.drop('movieId', axis=1)\n",
    "        y = X['rating']\n",
    "        X = X.drop('movieId', axis=1)\n",
    "        X = X.drop('rating', axis=1)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        \n",
    "        self.X_test  = self.X_test.to_numpy()\n",
    "        self.X_train = self.X_train.to_numpy()\n",
    "            \n",
    "        # Convert the data to PyTorch tensors\n",
    "        self.X_train = torch.tensor(self.X_train, dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(self.X_test, dtype=torch.float32)\n",
    "        \n",
    "        self.y_test = np.array(self.y_test)\n",
    "        self.y_train = np.array(self.y_train)\n",
    "        \n",
    "        self.y_test = torch.tensor(self.y_test, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(self.y_train, dtype=torch.float32)\n",
    "        \n",
    "        # Define the neural network architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, 1)\n",
    "        )\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        # Iterate over the entire training dataset multiple times\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            # Set the model to training mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Shuffle the training data\n",
    "            indices = torch.randperm(len(self.X_train))\n",
    "            shuffled_X = self.X_train[indices]\n",
    "            shuffled_y = self.y_train[indices]\n",
    "\n",
    "            # Divide the training data into batches\n",
    "            for i in range(0, len(self.X_train), self.batch_size):\n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Compute the forward pass\n",
    "                #batch_X = shuffled_X[i:i + self.batch_size]\n",
    "                #batch_y = shuffled_y[i:i + self.batch_size]\n",
    "                batch_X = self.X_train\n",
    "                batch_y = self.y_train\n",
    "                \n",
    "                outputs = self.model(batch_X)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                # Compute the backward pass and update the weights\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation data\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.model(self.X_test)\n",
    "                val_loss = self.criterion(val_outputs.squeeze(), self.y_test)\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            # Print the training and validation loss\n",
    "            #print(f'Epoch {epoch + 1}/{num_epochs} - eta: {elapsed_time:.2f}s -\\tTrain Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    def test(self):\n",
    "    # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Disable gradient calculation to speed up inference\n",
    "        with torch.no_grad():\n",
    "            # Compute the predictions for the test data\n",
    "            y_pred = self.model(self.X_test).squeeze()\n",
    "\n",
    "            # Compute the test loss\n",
    "            test_loss = self.criterion(y_pred, self.y_test)\n",
    "\n",
    "            # Compute the root mean squared error (RMSE) of the test predictions\n",
    "            rmse = torch.sqrt(test_loss)\n",
    "\n",
    "            # Compute the mean squared error (MSE) of the test predictions\n",
    "            mse = test_loss.item()\n",
    "\n",
    "            # Compute the mean absolute error (MAE) of the test predictions\n",
    "            mae = nn.functional.l1_loss(y_pred, self.y_test).item()\n",
    "\n",
    "            # Compute R^2 score of the test predictions\n",
    "            ss_res = torch.sum(torch.square(y_pred - self.y_test))\n",
    "            ss_tot = torch.sum(torch.square(self.y_test - torch.mean(self.y_test)))\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "            # Print the test metrics\n",
    "            #print(f'Test RMSE: {rmse.item():.4f}')\n",
    "            #print(f'Test MSE: {mse:.4f}')\n",
    "            #print(f'Test MAE: {mae:.4f}')\n",
    "            #print(f'Test R^2 score: {r2.item():.4f}')\n",
    "        return  r2.item(), self.model\n",
    "    \n",
    "    def load(self,model):\n",
    "        self.model =model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = NeuralNetwork(ratings, \\n                        genome_scores,\\n                        seed=1038893, \\n                        batch_size=128, \\n                        hidden_size1=64, \\n                        hidden_size2=64, \\n                        lr=0.0005)\\n\\n\\nmodel.train(100)\\nmodel.test()'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''model = NeuralNetwork(ratings, \n",
    "                        genome_scores,\n",
    "                        seed=1038893, \n",
    "                        batch_size=128, \n",
    "                        hidden_size1=64, \n",
    "                        hidden_size2=64, \n",
    "                        lr=0.0005)\n",
    "\n",
    "\n",
    "model.train(100)\n",
    "model.test()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 200\n",
      "DONE: 0.9534544348716736 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 200\n",
      "New best model: 128_64_64_0.0001_200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 500\n",
      "DONE: 0.9327754974365234 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 1000\n",
      "DONE: 0.915275514125824 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 200\n",
      "DONE: 0.9579471349716187 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 200\n",
      "New best model: 128_64_64_0.0005_200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 500\n",
      "DONE: 0.9453018307685852 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 1000\n",
      "DONE: 0.9386171698570251 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.0005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 200\n",
      "DONE: 0.962486743927002 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 200\n",
      "New best model: 128_64_64_0.001_200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 500\n",
      "DONE: 0.9543222188949585 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 1000\n",
      "DONE: 0.9420042037963867 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 200\n",
      "DONE: 0.9779330492019653 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 200\n",
      "New best model: 128_64_64_0.005_200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 500\n",
      "DONE: 0.9647583365440369 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 1000\n",
      "DONE: 0.9472816586494446 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 200\n",
      "DONE: 0.9682163596153259 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 500\n",
      "DONE: 0.9487801790237427 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 1000\n",
      "DONE: 0.9495113492012024 => batchsize: 128, hidden_size1: 64, hidden_size2: 64, lr: 0.01, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 200\n",
      "DONE: 0.9285963773727417 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 500\n",
      "DONE: 0.9352163076400757 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 1000\n",
      "DONE: 0.9226979613304138 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 200\n",
      "DONE: 0.9669129252433777 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 500\n",
      "DONE: 0.9529227614402771 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 1000\n",
      "DONE: 0.9548844695091248 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.0005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 200\n",
      "DONE: 0.9623223543167114 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 500\n",
      "DONE: 0.9635682702064514 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 1000\n",
      "DONE: 0.9522227048873901 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 200\n",
      "DONE: 0.9635456800460815 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 500\n",
      "DONE: 0.9778515100479126 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 1000\n",
      "DONE: 0.967368483543396 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 200\n",
      "DONE: 0.9716535210609436 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 500\n",
      "DONE: 0.9662293195724487 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 1000\n",
      "DONE: 0.9730761051177979 => batchsize: 128, hidden_size1: 64, hidden_size2: 128, lr: 0.01, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 200\n",
      "DONE: 0.9500279426574707 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 500\n",
      "DONE: 0.9289184212684631 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 1000\n",
      "DONE: 0.9285136461257935 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 200\n",
      "DONE: 0.9602906107902527 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 500\n",
      "DONE: 0.9603644609451294 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 1000\n",
      "DONE: 0.956593930721283 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.0005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 200\n",
      "DONE: 0.9706404805183411 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 500\n",
      "DONE: 0.9576453566551208 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 1000\n",
      "DONE: 0.9548322558403015 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 200\n",
      "DONE: 0.970565676689148 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 500\n",
      "DONE: 0.9616065621376038 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 1000\n",
      "DONE: 0.9741735458374023 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 200\n",
      "DONE: 0.9771025776863098 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 500\n",
      "DONE: 0.9711977243423462 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 1000\n",
      "DONE: 0.9704987406730652 => batchsize: 128, hidden_size1: 64, hidden_size2: 256, lr: 0.01, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 200\n",
      "DONE: 0.9244378209114075 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 500\n",
      "DONE: 0.9354737997055054 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 1000\n",
      "DONE: 0.9470373392105103 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 200\n",
      "DONE: 0.9644046425819397 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 500\n",
      "DONE: 0.9643305540084839 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 1000\n",
      "DONE: 0.9597102403640747 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.0005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 200\n",
      "DONE: 0.9737762212753296 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 500\n",
      "DONE: 0.9585790634155273 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 1000\n",
      "DONE: 0.9438897371292114 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 200\n",
      "DONE: 0.971613347530365 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 500\n",
      "DONE: 0.9616743326187134 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 1000\n",
      "DONE: 0.9733784198760986 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 200\n",
      "DONE: 0.9727979898452759 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 500\n",
      "DONE: 0.9774010181427002 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 1000\n",
      "DONE: 0.9785277843475342 => batchsize: 128, hidden_size1: 64, hidden_size2: 512, lr: 0.01, num_epochs: 1000\n",
      "New best model: 128_64_512_0.01_1000\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 200\n",
      "DONE: 0.9420328140258789 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 500\n",
      "DONE: 0.9346979260444641 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 1000\n",
      "DONE: 0.9328274130821228 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 200\n",
      "DONE: 0.9462772607803345 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 500\n",
      "DONE: 0.9480435252189636 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 1000\n",
      "DONE: 0.9503936767578125 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.0005, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 200\n",
      "DONE: 0.9578051567077637 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 500\n",
      "DONE: 0.9535700082778931 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 1000\n",
      "DONE: 0.9591875076293945 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.001, num_epochs: 1000\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.005, num_epochs: 200\n",
      "DONE: 0.9623128175735474 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.005, num_epochs: 200\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.005, num_epochs: 500\n",
      "DONE: 0.9610810279846191 => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.005, num_epochs: 500\n",
      "START => batchsize: 128, hidden_size1: 128, hidden_size2: 64, lr: 0.005, num_epochs: 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSTART => batchsize: \u001b[39m\u001b[39m{\u001b[39;00mbatchsize\u001b[39m}\u001b[39;00m\u001b[39m, hidden_size1: \u001b[39m\u001b[39m{\u001b[39;00mhidden_size1\u001b[39m}\u001b[39;00m\u001b[39m, hidden_size2: \u001b[39m\u001b[39m{\u001b[39;00mhidden_size2\u001b[39m}\u001b[39;00m\u001b[39m, lr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, num_epochs: \u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m model \u001b[39m=\u001b[39m NeuralNetwork(ratings, \n\u001b[0;32m     16\u001b[0m                     genome_scores,\n\u001b[0;32m     17\u001b[0m                     seed\u001b[39m=\u001b[39m\u001b[39m1038893\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m                     hidden_size2\u001b[39m=\u001b[39mhidden_size2, \n\u001b[0;32m     21\u001b[0m                     lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m---> 24\u001b[0m model\u001b[39m.\u001b[39;49mtrain(num_epochs)\n\u001b[0;32m     25\u001b[0m r2, model_instance  \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest()\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDONE: \u001b[39m\u001b[39m{\u001b[39;00mr2\u001b[39m}\u001b[39;00m\u001b[39m => batchsize: \u001b[39m\u001b[39m{\u001b[39;00mbatchsize\u001b[39m}\u001b[39;00m\u001b[39m, hidden_size1: \u001b[39m\u001b[39m{\u001b[39;00mhidden_size1\u001b[39m}\u001b[39;00m\u001b[39m, hidden_size2: \u001b[39m\u001b[39m{\u001b[39;00mhidden_size2\u001b[39m}\u001b[39;00m\u001b[39m, lr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, num_epochs: \u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 75\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m     72\u001b[0m batch_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train\n\u001b[0;32m     73\u001b[0m batch_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train\n\u001b[1;32m---> 75\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_X)\n\u001b[0;32m     77\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[0;32m     78\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs\u001b[39m.\u001b[39msqueeze(), batch_y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "batchsize = [128,256,512,1024]\t\n",
    "hidden_size1 = [64,128,256]\n",
    "hidden_size2 = [64,128,256]\n",
    "lr = [0.0001,0.001,0.01]\n",
    "num_epochs = [200,400,600]\n",
    "\n",
    "\n",
    "best_model_params = None \n",
    "best_r2 = 0\n",
    "\n",
    "for batchsize,hidden_size1,hidden_size2,lr,num_epochs in itertools.product(batchsize,hidden_size1,hidden_size2,lr,num_epochs):\n",
    "    print(f'START => batchsize: {batchsize}, hidden_size1: {hidden_size1}, hidden_size2: {hidden_size2}, lr: {lr}, num_epochs: {num_epochs}')\n",
    "    model = NeuralNetwork(ratings, \n",
    "                        genome_scores,\n",
    "                        seed=1038893, \n",
    "                        batch_size=batchsize, \n",
    "                        hidden_size1=hidden_size1, \n",
    "                        hidden_size2=hidden_size2, \n",
    "                        lr=lr)\n",
    "    \n",
    "    \n",
    "    model.train(num_epochs)\n",
    "    r2, model_instance  = model.test()\n",
    "    print(f'DONE: {r2} => batchsize: {batchsize}, hidden_size1: {hidden_size1}, hidden_size2: {hidden_size2}, lr: {lr}, num_epochs: {num_epochs}')\n",
    "    \n",
    "    #Se lo voglio caricare uso torch.load('modello.pt')\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model_params = f'{batchsize}_{hidden_size1}_{hidden_size2}_{lr}_{num_epochs}'\n",
    "        torch.save(model_instance, f'model\\model_{batchsize}_{hidden_size1}_{hidden_size2}_{lr}_{num_epochs}.pt')\n",
    "        print(f'New best model: {best_model_params}')\n",
    "    \n",
    "print('============================================================')\n",
    "model = NeuralNetwork(ratings,genome_scores)\n",
    "print(f'Best model SCORE: {best_r2}')\n",
    "model.load(torch.load(f'model/model_{best_model_params}.pt'));\n",
    "print(model.test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#model = NeuralNetwork(ratings,genome_scores)\n",
    "#model.load(torch.load('model/model_128_64_64_0.0001_10.pt'));\n",
    "#print(model.test())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87bd6c09cddab90a09f19a55e9245e3497687050bc7d26cfabaa798d009918f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
