{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "        \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1038893\n",
    "\n",
    "fix_random(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "root = \"../../data/ml-25m\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(file: str, nrows=None):\n",
    "    if nrows:\n",
    "        df = pd.read_csv(f\"{root}/{file}\", nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{root}/{file}\")\n",
    "    print(f\"Loaded ml-25m data: {root}/{file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ratings_from_csv(nrows=None):\n",
    "    path = \"ratings.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "\n",
    "    data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "    # todo: drop user id\n",
    "    #data.drop(\"userId\", axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_tag_relevances_from_csv(nrows=None):\n",
    "    path = \"genome-scores.csv\"\n",
    "\n",
    "    return get_data_from_csv(path, nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores_path = \"genome-scores.csv\"\n",
    "ratings_path = \"ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/ratings.csv\n",
      "          userId  movieId  rating\n",
      "0              1      296     5.0\n",
      "1              1      306     3.5\n",
      "2              1      307     5.0\n",
      "3              1      665     5.0\n",
      "4              1      899     3.5\n",
      "...          ...      ...     ...\n",
      "25000090  162541    50872     4.5\n",
      "25000091  162541    55768     2.5\n",
      "25000092  162541    56176     2.0\n",
      "25000093  162541    58559     4.0\n",
      "25000094  162541    63876     5.0\n",
      "\n",
      "[25000095 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ratings = get_ratings_from_csv()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/genome-scores.csv\n",
      "          movieId  tagId  relevance\n",
      "0               1      1    0.02875\n",
      "1               1      2    0.02375\n",
      "2               1      3    0.06250\n",
      "3               1      4    0.07575\n",
      "4               1      5    0.14075\n",
      "...           ...    ...        ...\n",
      "15584443   206499   1124    0.11000\n",
      "15584444   206499   1125    0.04850\n",
      "15584445   206499   1126    0.01325\n",
      "15584446   206499   1127    0.14025\n",
      "15584447   206499   1128    0.03350\n",
      "\n",
      "[15584448 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "genome_scores = get_tag_relevances_from_csv()\n",
    "print(genome_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# films = get_data_from_csv(f\"{root}/{ratings}\")[]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # Fill in missing values with zeros\n",
    "            # X.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnOperation(ratings,X):\n",
    "     # Compute the mean rating for each user\n",
    "     count_rating = ratings.groupby('movieId', as_index=False)['rating'].count()\n",
    "     std= ratings.groupby('movieId', as_index=False)['rating'].std()\n",
    "     std.fillna(0, inplace=True)\n",
    "     min_ratings= ratings.groupby('movieId', as_index=False)['rating'].min()\n",
    "     max_ratings= ratings.groupby('movieId', as_index=False)['rating'].max()\n",
    "     median= ratings.groupby('movieId', as_index=False)['rating'].median()\n",
    "     operation = pd.DataFrame({'movieId':count_rating['movieId'],'count_rating': count_rating['rating'], 'std': std['rating'], 'min': min_ratings['rating'], 'max': max_ratings['rating'], 'median': median['rating']}) \n",
    "     X = pd.merge(X, operation, on='movieId')\n",
    "     X.drop(\"movieId\", axis=1, inplace=True)\n",
    "     return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARE TEST CON AVG, STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ratings, relevance, seed=1038893, batch_size=32, hidden_size1=64, hidden_size2=32, lr=0.001):\n",
    "        # Pivot the relevance DataFrame to create a matrix of tag relevance scores for each movie\n",
    "        relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "\n",
    "        # Compute the mean rating for each user\n",
    "        mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "\n",
    "        # Merge the ratings and relevance data\n",
    "        X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "        X = addColumnOperation(ratings,X)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        ratings = None\n",
    "\n",
    "        #X = X.drop('movieId', axis=1)\n",
    "        y = X['rating']\n",
    "        X = X.drop('rating', axis=1)\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        # Convert the data to PyTorch tensors\n",
    "        self.X_train = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(self.y_train.values, dtype=torch.float32)\n",
    "        self.X_test = torch.tensor(self.X_test.values, dtype=torch.float32)\n",
    "        self.y_test = torch.tensor(self.y_test.values, dtype=torch.float32)\n",
    "\n",
    "        # Define the neural network architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(X.shape[1], hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, 1)\n",
    "        )\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        # Iterate over the entire training dataset multiple times\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            # Set the model to training mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Shuffle the training data\n",
    "            indices = torch.randperm(len(self.X_train))\n",
    "            shuffled_X = self.X_train[indices]\n",
    "            shuffled_y = self.y_train[indices]\n",
    "\n",
    "            # Divide the training data into batches\n",
    "            for i in range(0, len(self.X_train), self.batch_size):\n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Compute the forward pass\n",
    "                #batch_X = shuffled_X[i:i + self.batch_size]\n",
    "                #batch_y = shuffled_y[i:i + self.batch_size]\n",
    "                batch_X = self.X_train\n",
    "                batch_y = self.y_train\n",
    "                \n",
    "                outputs = self.model(batch_X)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                # Compute the backward pass and update the weights\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation data\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.model(self.X_test)\n",
    "                val_loss = self.criterion(val_outputs.squeeze(), self.y_test)\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            # Print the training and validation loss\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} - eta: {elapsed_time:.2f}s -\\tTrain Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    def test(self):\n",
    "    # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Disable gradient calculation to speed up inference\n",
    "        with torch.no_grad():\n",
    "            # Compute the predictions for the test data\n",
    "            y_pred = self.model(self.X_test).squeeze()\n",
    "\n",
    "            # Compute the test loss\n",
    "            test_loss = self.criterion(y_pred, self.y_test)\n",
    "\n",
    "            # Compute the root mean squared error (RMSE) of the test predictions\n",
    "            rmse = torch.sqrt(test_loss)\n",
    "\n",
    "            # Compute the mean squared error (MSE) of the test predictions\n",
    "            mse = test_loss.item()\n",
    "\n",
    "            # Compute the mean absolute error (MAE) of the test predictions\n",
    "            mae = nn.functional.l1_loss(y_pred, self.y_test).item()\n",
    "\n",
    "            # Compute R^2 score of the test predictions\n",
    "            ss_res = torch.sum(torch.square(y_pred - self.y_test))\n",
    "            ss_tot = torch.sum(torch.square(self.y_test - torch.mean(self.y_test)))\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "            # Print the test metrics\n",
    "            print(f'Test RMSE: {rmse.item():.4f}')\n",
    "            print(f'Test MSE: {mse:.4f}')\n",
    "            print(f'Test MAE: {mae:.4f}')\n",
    "            print(f'Test R^2 score: {r2.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(ratings, \n",
    "                        genome_scores,\n",
    "                        seed=1038893, \n",
    "                        batch_size=64, \n",
    "                        hidden_size1=64, \n",
    "                        hidden_size2=64, \n",
    "                        lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - eta: 7.63s -\tTrain Loss: 0.3729, Val Loss: 0.3683\n",
      "Epoch 2/200 - eta: 7.37s -\tTrain Loss: 0.2692, Val Loss: 0.2653\n",
      "Epoch 3/200 - eta: 7.23s -\tTrain Loss: 0.1751, Val Loss: 0.1635\n",
      "Epoch 4/200 - eta: 7.00s -\tTrain Loss: 0.0542, Val Loss: 0.0549\n",
      "Epoch 5/200 - eta: 7.57s -\tTrain Loss: 0.0623, Val Loss: 0.0640\n",
      "Epoch 6/200 - eta: 7.84s -\tTrain Loss: 0.0342, Val Loss: 0.0359\n",
      "Epoch 7/200 - eta: 7.39s -\tTrain Loss: 0.0183, Val Loss: 0.0192\n",
      "Epoch 8/200 - eta: 8.94s -\tTrain Loss: 0.0163, Val Loss: 0.0171\n",
      "Epoch 9/200 - eta: 8.64s -\tTrain Loss: 0.0110, Val Loss: 0.0119\n",
      "Epoch 10/200 - eta: 8.61s -\tTrain Loss: 0.0832, Val Loss: 0.1228\n",
      "Epoch 11/200 - eta: 8.95s -\tTrain Loss: 0.0134, Val Loss: 0.0143\n",
      "Epoch 12/200 - eta: 9.06s -\tTrain Loss: 0.0083, Val Loss: 0.0094\n",
      "Epoch 13/200 - eta: 8.24s -\tTrain Loss: 0.0088, Val Loss: 0.0096\n",
      "Epoch 14/200 - eta: 8.00s -\tTrain Loss: 0.0098, Val Loss: 0.0109\n",
      "Epoch 15/200 - eta: 8.19s -\tTrain Loss: 0.0098, Val Loss: 0.0108\n",
      "Epoch 16/200 - eta: 7.99s -\tTrain Loss: 0.0101, Val Loss: 0.0110\n",
      "Epoch 17/200 - eta: 8.03s -\tTrain Loss: 0.0089, Val Loss: 0.0111\n",
      "Epoch 18/200 - eta: 7.92s -\tTrain Loss: 0.0078, Val Loss: 0.0089\n",
      "Epoch 19/200 - eta: 8.06s -\tTrain Loss: 0.0085, Val Loss: 0.0096\n",
      "Epoch 20/200 - eta: 8.13s -\tTrain Loss: 0.0099, Val Loss: 0.0104\n",
      "Epoch 21/200 - eta: 8.15s -\tTrain Loss: 0.0173, Val Loss: 0.0704\n",
      "Epoch 22/200 - eta: 8.22s -\tTrain Loss: 0.0090, Val Loss: 0.0120\n",
      "Epoch 23/200 - eta: 18.81s -\tTrain Loss: 0.1018, Val Loss: 0.0467\n",
      "Epoch 24/200 - eta: 11.32s -\tTrain Loss: 0.0062, Val Loss: 0.0084\n",
      "Epoch 25/200 - eta: 8.53s -\tTrain Loss: 0.0318, Val Loss: 0.0112\n",
      "Epoch 26/200 - eta: 7.26s -\tTrain Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch 27/200 - eta: 7.06s -\tTrain Loss: 0.0096, Val Loss: 0.0116\n",
      "Epoch 28/200 - eta: 7.56s -\tTrain Loss: 0.0063, Val Loss: 0.0077\n",
      "Epoch 29/200 - eta: 7.38s -\tTrain Loss: 0.0057, Val Loss: 0.0069\n",
      "Epoch 30/200 - eta: 8.03s -\tTrain Loss: 0.0106, Val Loss: 0.0452\n",
      "Epoch 31/200 - eta: 7.13s -\tTrain Loss: 0.0063, Val Loss: 0.0084\n",
      "Epoch 32/200 - eta: 8.50s -\tTrain Loss: 0.0055, Val Loss: 0.0068\n",
      "Epoch 33/200 - eta: 7.69s -\tTrain Loss: 0.0091, Val Loss: 0.0163\n",
      "Epoch 34/200 - eta: 7.03s -\tTrain Loss: 0.0616, Val Loss: 0.0128\n",
      "Epoch 35/200 - eta: 6.88s -\tTrain Loss: 0.0053, Val Loss: 0.0066\n",
      "Epoch 36/200 - eta: 8.06s -\tTrain Loss: 0.0052, Val Loss: 0.0065\n",
      "Epoch 37/200 - eta: 8.32s -\tTrain Loss: 0.0051, Val Loss: 0.0065\n",
      "Epoch 38/200 - eta: 7.29s -\tTrain Loss: 0.0050, Val Loss: 0.0063\n",
      "Epoch 39/200 - eta: 7.69s -\tTrain Loss: 0.0048, Val Loss: 0.0062\n",
      "Epoch 40/200 - eta: 7.30s -\tTrain Loss: 0.0053, Val Loss: 0.0068\n",
      "Epoch 41/200 - eta: 7.78s -\tTrain Loss: 0.0049, Val Loss: 0.0063\n",
      "Epoch 42/200 - eta: 7.43s -\tTrain Loss: 0.0067, Val Loss: 0.0260\n",
      "Epoch 43/200 - eta: 7.63s -\tTrain Loss: 0.0056, Val Loss: 0.0077\n",
      "Epoch 44/200 - eta: 8.07s -\tTrain Loss: 0.0055, Val Loss: 0.0072\n",
      "Epoch 45/200 - eta: 10.41s -\tTrain Loss: 0.0050, Val Loss: 0.0067\n",
      "Epoch 46/200 - eta: 8.61s -\tTrain Loss: 0.0046, Val Loss: 0.0061\n",
      "Epoch 47/200 - eta: 7.04s -\tTrain Loss: 0.0045, Val Loss: 0.0060\n",
      "Epoch 48/200 - eta: 7.60s -\tTrain Loss: 0.0046, Val Loss: 0.0064\n",
      "Epoch 49/200 - eta: 8.20s -\tTrain Loss: 0.0045, Val Loss: 0.0061\n",
      "Epoch 50/200 - eta: 7.52s -\tTrain Loss: 0.0043, Val Loss: 0.0059\n",
      "Epoch 51/200 - eta: 6.94s -\tTrain Loss: 0.0129, Val Loss: 0.0381\n",
      "Epoch 52/200 - eta: 7.25s -\tTrain Loss: 0.0053, Val Loss: 0.0068\n",
      "Epoch 53/200 - eta: 7.10s -\tTrain Loss: 0.0041, Val Loss: 0.0059\n",
      "Epoch 54/200 - eta: 7.19s -\tTrain Loss: 0.0220, Val Loss: 0.0111\n",
      "Epoch 55/200 - eta: 7.16s -\tTrain Loss: 0.0042, Val Loss: 0.0063\n",
      "Epoch 56/200 - eta: 7.10s -\tTrain Loss: 0.0066, Val Loss: 0.0111\n",
      "Epoch 57/200 - eta: 6.88s -\tTrain Loss: 0.0040, Val Loss: 0.0060\n",
      "Epoch 58/200 - eta: 7.14s -\tTrain Loss: 0.0040, Val Loss: 0.0064\n",
      "Epoch 59/200 - eta: 6.89s -\tTrain Loss: 0.0200, Val Loss: 0.0214\n",
      "Epoch 60/200 - eta: 7.42s -\tTrain Loss: 0.0048, Val Loss: 0.0080\n",
      "Epoch 61/200 - eta: 7.19s -\tTrain Loss: 0.0040, Val Loss: 0.0062\n",
      "Epoch 62/200 - eta: 6.99s -\tTrain Loss: 0.0048, Val Loss: 0.0117\n",
      "Epoch 63/200 - eta: 7.22s -\tTrain Loss: 0.0036, Val Loss: 0.0059\n",
      "Epoch 64/200 - eta: 7.11s -\tTrain Loss: 0.0036, Val Loss: 0.0061\n",
      "Epoch 65/200 - eta: 6.78s -\tTrain Loss: 0.0041, Val Loss: 0.0072\n",
      "Epoch 66/200 - eta: 6.99s -\tTrain Loss: 0.0042, Val Loss: 0.0071\n",
      "Epoch 67/200 - eta: 7.02s -\tTrain Loss: 0.0081, Val Loss: 0.0073\n",
      "Epoch 68/200 - eta: 7.11s -\tTrain Loss: 0.0047, Val Loss: 0.0080\n",
      "Epoch 69/200 - eta: 9.99s -\tTrain Loss: 0.0126, Val Loss: 0.0168\n",
      "Epoch 70/200 - eta: 8.71s -\tTrain Loss: 0.0056, Val Loss: 0.0090\n",
      "Epoch 71/200 - eta: 7.81s -\tTrain Loss: 0.0032, Val Loss: 0.0059\n",
      "Epoch 72/200 - eta: 7.51s -\tTrain Loss: 0.0045, Val Loss: 0.0072\n",
      "Epoch 73/200 - eta: 7.23s -\tTrain Loss: 0.0058, Val Loss: 0.0097\n",
      "Epoch 74/200 - eta: 7.34s -\tTrain Loss: 0.0034, Val Loss: 0.0064\n",
      "Epoch 75/200 - eta: 8.01s -\tTrain Loss: 0.0085, Val Loss: 0.0187\n",
      "Epoch 76/200 - eta: 8.70s -\tTrain Loss: 0.0087, Val Loss: 0.0119\n",
      "Epoch 77/200 - eta: 7.35s -\tTrain Loss: 0.0053, Val Loss: 0.0080\n",
      "Epoch 78/200 - eta: 7.64s -\tTrain Loss: 0.0039, Val Loss: 0.0073\n",
      "Epoch 79/200 - eta: 8.44s -\tTrain Loss: 0.0032, Val Loss: 0.0071\n",
      "Epoch 80/200 - eta: 9.22s -\tTrain Loss: 0.0033, Val Loss: 0.0066\n",
      "Epoch 81/200 - eta: 8.86s -\tTrain Loss: 0.0030, Val Loss: 0.0061\n",
      "Epoch 82/200 - eta: 8.54s -\tTrain Loss: 0.0033, Val Loss: 0.0072\n",
      "Epoch 83/200 - eta: 9.93s -\tTrain Loss: 0.0249, Val Loss: 0.0248\n",
      "Epoch 84/200 - eta: 9.23s -\tTrain Loss: 0.0105, Val Loss: 0.0116\n",
      "Epoch 85/200 - eta: 9.70s -\tTrain Loss: 0.0173, Val Loss: 0.0166\n",
      "Epoch 86/200 - eta: 12.05s -\tTrain Loss: 0.0059, Val Loss: 0.0073\n",
      "Epoch 87/200 - eta: 13.05s -\tTrain Loss: 0.0195, Val Loss: 0.0193\n",
      "Epoch 88/200 - eta: 13.79s -\tTrain Loss: 0.0196, Val Loss: 0.0182\n",
      "Epoch 89/200 - eta: 11.62s -\tTrain Loss: 0.0050, Val Loss: 0.0066\n",
      "Epoch 90/200 - eta: 8.69s -\tTrain Loss: 0.0098, Val Loss: 0.0141\n",
      "Epoch 91/200 - eta: 8.37s -\tTrain Loss: 0.0189, Val Loss: 0.0249\n",
      "Epoch 92/200 - eta: 8.75s -\tTrain Loss: 0.0058, Val Loss: 0.0064\n",
      "Epoch 93/200 - eta: 9.27s -\tTrain Loss: 0.0055, Val Loss: 0.0066\n",
      "Epoch 94/200 - eta: 7.16s -\tTrain Loss: 0.0041, Val Loss: 0.0059\n",
      "Epoch 95/200 - eta: 7.00s -\tTrain Loss: 0.0044, Val Loss: 0.0073\n",
      "Epoch 96/200 - eta: 7.44s -\tTrain Loss: 0.0439, Val Loss: 0.0341\n",
      "Epoch 97/200 - eta: 5.99s -\tTrain Loss: 0.0037, Val Loss: 0.0056\n",
      "Epoch 98/200 - eta: 7.10s -\tTrain Loss: 0.0038, Val Loss: 0.0056\n",
      "Epoch 99/200 - eta: 6.58s -\tTrain Loss: 0.0054, Val Loss: 0.0074\n",
      "Epoch 100/200 - eta: 6.52s -\tTrain Loss: 0.0394, Val Loss: 0.0659\n",
      "Epoch 101/200 - eta: 6.49s -\tTrain Loss: 0.0037, Val Loss: 0.0057\n",
      "Epoch 102/200 - eta: 6.49s -\tTrain Loss: 0.0034, Val Loss: 0.0055\n",
      "Epoch 103/200 - eta: 6.11s -\tTrain Loss: 0.0089, Val Loss: 0.0095\n",
      "Epoch 104/200 - eta: 6.45s -\tTrain Loss: 0.0037, Val Loss: 0.0063\n",
      "Epoch 105/200 - eta: 7.16s -\tTrain Loss: 0.0033, Val Loss: 0.0056\n",
      "Epoch 106/200 - eta: 7.16s -\tTrain Loss: 0.0045, Val Loss: 0.0071\n",
      "Epoch 107/200 - eta: 7.21s -\tTrain Loss: 0.0031, Val Loss: 0.0055\n",
      "Epoch 108/200 - eta: 7.52s -\tTrain Loss: 0.0032, Val Loss: 0.0056\n",
      "Epoch 109/200 - eta: 6.86s -\tTrain Loss: 0.0046, Val Loss: 0.0075\n",
      "Epoch 110/200 - eta: 6.50s -\tTrain Loss: 0.0157, Val Loss: 0.0057\n",
      "Epoch 111/200 - eta: 5.94s -\tTrain Loss: 0.0030, Val Loss: 0.0055\n",
      "Epoch 112/200 - eta: 5.97s -\tTrain Loss: 0.0061, Val Loss: 0.0073\n",
      "Epoch 113/200 - eta: 5.77s -\tTrain Loss: 0.0029, Val Loss: 0.0055\n",
      "Epoch 114/200 - eta: 6.05s -\tTrain Loss: 0.0047, Val Loss: 0.0103\n",
      "Epoch 115/200 - eta: 7.17s -\tTrain Loss: 0.0030, Val Loss: 0.0057\n",
      "Epoch 116/200 - eta: 7.81s -\tTrain Loss: 0.0028, Val Loss: 0.0056\n",
      "Epoch 117/200 - eta: 7.87s -\tTrain Loss: 0.0042, Val Loss: 0.0095\n",
      "Epoch 118/200 - eta: 6.83s -\tTrain Loss: 0.0029, Val Loss: 0.0058\n",
      "Epoch 119/200 - eta: 6.45s -\tTrain Loss: 0.0028, Val Loss: 0.0058\n",
      "Epoch 120/200 - eta: 6.96s -\tTrain Loss: 0.0027, Val Loss: 0.0057\n",
      "Epoch 121/200 - eta: 7.63s -\tTrain Loss: 0.0061, Val Loss: 0.0128\n",
      "Epoch 122/200 - eta: 7.11s -\tTrain Loss: 0.0059, Val Loss: 0.0068\n",
      "Epoch 123/200 - eta: 8.10s -\tTrain Loss: 0.0030, Val Loss: 0.0065\n",
      "Epoch 124/200 - eta: 6.99s -\tTrain Loss: 0.0027, Val Loss: 0.0059\n",
      "Epoch 125/200 - eta: 6.59s -\tTrain Loss: 0.0027, Val Loss: 0.0059\n",
      "Epoch 126/200 - eta: 6.64s -\tTrain Loss: 0.0072, Val Loss: 0.0092\n",
      "Epoch 127/200 - eta: 6.80s -\tTrain Loss: 0.0304, Val Loss: 0.0322\n",
      "Epoch 128/200 - eta: 6.76s -\tTrain Loss: 0.0062, Val Loss: 0.0084\n",
      "Epoch 129/200 - eta: 6.84s -\tTrain Loss: 0.0023, Val Loss: 0.0057\n",
      "Epoch 130/200 - eta: 8.30s -\tTrain Loss: 0.0050, Val Loss: 0.0097\n",
      "Epoch 131/200 - eta: 12.38s -\tTrain Loss: 0.0037, Val Loss: 0.0067\n",
      "Epoch 132/200 - eta: 10.41s -\tTrain Loss: 0.0037, Val Loss: 0.0088\n",
      "Epoch 133/200 - eta: 10.96s -\tTrain Loss: 0.0023, Val Loss: 0.0058\n",
      "Epoch 134/200 - eta: 8.92s -\tTrain Loss: 0.0025, Val Loss: 0.0060\n",
      "Epoch 135/200 - eta: 6.84s -\tTrain Loss: 0.0021, Val Loss: 0.0058\n",
      "Epoch 136/200 - eta: 6.26s -\tTrain Loss: 0.0022, Val Loss: 0.0058\n",
      "Epoch 137/200 - eta: 6.63s -\tTrain Loss: 0.0071, Val Loss: 0.0071\n",
      "Epoch 138/200 - eta: 6.56s -\tTrain Loss: 0.0021, Val Loss: 0.0058\n",
      "Epoch 139/200 - eta: 7.34s -\tTrain Loss: 0.0021, Val Loss: 0.0059\n",
      "Epoch 140/200 - eta: 9.24s -\tTrain Loss: 0.0021, Val Loss: 0.0059\n",
      "Epoch 141/200 - eta: 9.81s -\tTrain Loss: 0.0020, Val Loss: 0.0059\n",
      "Epoch 142/200 - eta: 9.81s -\tTrain Loss: 0.0029, Val Loss: 0.0077\n",
      "Epoch 143/200 - eta: 9.97s -\tTrain Loss: 0.0021, Val Loss: 0.0060\n",
      "Epoch 144/200 - eta: 10.28s -\tTrain Loss: 0.0021, Val Loss: 0.0064\n",
      "Epoch 145/200 - eta: 10.24s -\tTrain Loss: 0.0019, Val Loss: 0.0059\n",
      "Epoch 146/200 - eta: 11.61s -\tTrain Loss: 0.0033, Val Loss: 0.0075\n",
      "Epoch 147/200 - eta: 13.52s -\tTrain Loss: 0.0019, Val Loss: 0.0060\n",
      "Epoch 148/200 - eta: 18.72s -\tTrain Loss: 0.0025, Val Loss: 0.0073\n",
      "Epoch 149/200 - eta: 12.24s -\tTrain Loss: 0.0027, Val Loss: 0.0075\n",
      "Epoch 150/200 - eta: 12.70s -\tTrain Loss: 0.0018, Val Loss: 0.0061\n",
      "Epoch 151/200 - eta: 14.08s -\tTrain Loss: 0.0018, Val Loss: 0.0061\n",
      "Epoch 152/200 - eta: 14.28s -\tTrain Loss: 0.0018, Val Loss: 0.0062\n",
      "Epoch 153/200 - eta: 10.15s -\tTrain Loss: 0.0017, Val Loss: 0.0062\n",
      "Epoch 154/200 - eta: 10.81s -\tTrain Loss: 0.0019, Val Loss: 0.0066\n",
      "Epoch 155/200 - eta: 14.32s -\tTrain Loss: 0.0017, Val Loss: 0.0063\n",
      "Epoch 156/200 - eta: 12.92s -\tTrain Loss: 0.0045, Val Loss: 0.0078\n",
      "Epoch 157/200 - eta: 13.34s -\tTrain Loss: 0.0056, Val Loss: 0.0121\n",
      "Epoch 158/200 - eta: 11.94s -\tTrain Loss: 0.0021, Val Loss: 0.0064\n",
      "Epoch 159/200 - eta: 12.64s -\tTrain Loss: 0.0037, Val Loss: 0.0091\n",
      "Epoch 160/200 - eta: 14.45s -\tTrain Loss: 0.0088, Val Loss: 0.0155\n",
      "Epoch 161/200 - eta: 18.09s -\tTrain Loss: 0.0026, Val Loss: 0.0078\n",
      "Epoch 162/200 - eta: 14.79s -\tTrain Loss: 0.0029, Val Loss: 0.0063\n",
      "Epoch 163/200 - eta: 13.15s -\tTrain Loss: 0.0173, Val Loss: 0.0196\n",
      "Epoch 164/200 - eta: 12.98s -\tTrain Loss: 0.0015, Val Loss: 0.0064\n",
      "Epoch 165/200 - eta: 12.04s -\tTrain Loss: 0.0018, Val Loss: 0.0067\n",
      "Epoch 166/200 - eta: 11.82s -\tTrain Loss: 0.0041, Val Loss: 0.0061\n",
      "Epoch 167/200 - eta: 12.39s -\tTrain Loss: 0.0034, Val Loss: 0.0058\n",
      "Epoch 168/200 - eta: 15.70s -\tTrain Loss: 0.0031, Val Loss: 0.0056\n",
      "Epoch 169/200 - eta: 16.89s -\tTrain Loss: 0.0060, Val Loss: 0.0078\n",
      "Epoch 170/200 - eta: 14.38s -\tTrain Loss: 0.0027, Val Loss: 0.0055\n",
      "Epoch 171/200 - eta: 11.64s -\tTrain Loss: 0.0066, Val Loss: 0.0100\n",
      "Epoch 172/200 - eta: 11.85s -\tTrain Loss: 0.0025, Val Loss: 0.0058\n",
      "Epoch 173/200 - eta: 11.82s -\tTrain Loss: 0.0024, Val Loss: 0.0055\n",
      "Epoch 174/200 - eta: 12.08s -\tTrain Loss: 0.0030, Val Loss: 0.0064\n",
      "Epoch 175/200 - eta: 12.77s -\tTrain Loss: 0.0038, Val Loss: 0.0075\n",
      "Epoch 176/200 - eta: 12.03s -\tTrain Loss: 0.0022, Val Loss: 0.0055\n",
      "Epoch 177/200 - eta: 12.44s -\tTrain Loss: 0.0021, Val Loss: 0.0055\n",
      "Epoch 178/200 - eta: 19.39s -\tTrain Loss: 0.0080, Val Loss: 0.0099\n",
      "Epoch 179/200 - eta: 15.28s -\tTrain Loss: 0.0028, Val Loss: 0.0065\n",
      "Epoch 180/200 - eta: 16.03s -\tTrain Loss: 0.0020, Val Loss: 0.0055\n",
      "Epoch 181/200 - eta: 12.42s -\tTrain Loss: 0.0021, Val Loss: 0.0056\n",
      "Epoch 182/200 - eta: 12.68s -\tTrain Loss: 0.0020, Val Loss: 0.0058\n",
      "Epoch 183/200 - eta: 18.03s -\tTrain Loss: 0.0025, Val Loss: 0.0064\n",
      "Epoch 184/200 - eta: 19.63s -\tTrain Loss: 0.0023, Val Loss: 0.0057\n",
      "Epoch 185/200 - eta: 19.27s -\tTrain Loss: 0.0023, Val Loss: 0.0063\n",
      "Epoch 186/200 - eta: 9.11s -\tTrain Loss: 0.0020, Val Loss: 0.0059\n",
      "Epoch 187/200 - eta: 8.70s -\tTrain Loss: 0.0021, Val Loss: 0.0060\n",
      "Epoch 188/200 - eta: 8.08s -\tTrain Loss: 0.0034, Val Loss: 0.0062\n",
      "Epoch 189/200 - eta: 7.52s -\tTrain Loss: 0.0054, Val Loss: 0.0093\n",
      "Epoch 190/200 - eta: 6.88s -\tTrain Loss: 0.0026, Val Loss: 0.0058\n",
      "Epoch 191/200 - eta: 6.50s -\tTrain Loss: 0.0017, Val Loss: 0.0059\n",
      "Epoch 192/200 - eta: 7.03s -\tTrain Loss: 0.0016, Val Loss: 0.0058\n",
      "Epoch 193/200 - eta: 7.85s -\tTrain Loss: 0.0042, Val Loss: 0.0094\n",
      "Epoch 194/200 - eta: 7.58s -\tTrain Loss: 0.0021, Val Loss: 0.0063\n",
      "Epoch 195/200 - eta: 7.72s -\tTrain Loss: 0.0017, Val Loss: 0.0062\n",
      "Epoch 196/200 - eta: 8.10s -\tTrain Loss: 0.0017, Val Loss: 0.0062\n",
      "Epoch 197/200 - eta: 8.75s -\tTrain Loss: 0.0031, Val Loss: 0.0078\n",
      "Epoch 198/200 - eta: 6.33s -\tTrain Loss: 0.0029, Val Loss: 0.0083\n",
      "Epoch 199/200 - eta: 6.22s -\tTrain Loss: 0.0062, Val Loss: 0.0125\n",
      "Epoch 200/200 - eta: 8.54s -\tTrain Loss: 0.0023, Val Loss: 0.0073\n"
     ]
    }
   ],
   "source": [
    "model.train(num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.0853\n",
      "Test MSE: 0.0073\n",
      "Test MAE: 0.0637\n",
      "Test R^2 score: 0.9684\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87bd6c09cddab90a09f19a55e9245e3497687050bc7d26cfabaa798d009918f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
