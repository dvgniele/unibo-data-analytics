{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 1038893\n",
    "\n",
    "fix_random(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "root = \"../../data/ml-25m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Acquisition\n",
    "def get_data_from_csv(file: str, nrows=None):\n",
    "    if nrows:\n",
    "        df = pd.read_csv(f\"{root}/{file}\", nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{root}/{file}\")\n",
    "    print(f\"Loaded ml-25m data: {root}/{file}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings_from_csv(nrows=None):\n",
    "    path = \"ratings.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "\n",
    "    data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "    # todo: drop user id\n",
    "    #data.drop(\"userId\", axis=1, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/ratings.csv\n",
      "          userId  movieId  rating\n",
      "0              1      296     5.0\n",
      "1              1      306     3.5\n",
      "2              1      307     5.0\n",
      "3              1      665     5.0\n",
      "4              1      899     3.5\n",
      "...          ...      ...     ...\n",
      "25000090  162541    50872     4.5\n",
      "25000091  162541    55768     2.5\n",
      "25000092  162541    56176     2.0\n",
      "25000093  162541    58559     4.0\n",
      "25000094  162541    63876     5.0\n",
      "\n",
      "[25000095 rows x 3 columns]\n",
      "Loaded ml-25m data: ../../data/ml-25m/genome-scores.csv\n",
      "          movieId  tagId  relevance\n",
      "0               1      1    0.02875\n",
      "1               1      2    0.02375\n",
      "2               1      3    0.06250\n",
      "3               1      4    0.07575\n",
      "4               1      5    0.14075\n",
      "...           ...    ...        ...\n",
      "15584443   206499   1124    0.11000\n",
      "15584444   206499   1125    0.04850\n",
      "15584445   206499   1126    0.01325\n",
      "15584446   206499   1127    0.14025\n",
      "15584447   206499   1128    0.03350\n",
      "\n",
      "[15584448 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_tag_relevances_from_csv(nrows=None):\n",
    "    path = \"genome-scores.csv\"\n",
    "\n",
    "    return get_data_from_csv(path, nrows)\n",
    "genome_scores_path = \"genome-scores.csv\"\n",
    "ratings_path = \"ratings.csv\"\n",
    "ratings = get_ratings_from_csv()\n",
    "print(ratings)\n",
    "genome_scores = get_tag_relevances_from_csv()\n",
    "print(genome_scores)\n",
    "# films = get_data_from_csv(f\"{root}/{ratings}\")[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pytorch_tabnet.augmentations import RegressionSMOTE\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "           \n",
    "\n",
    "class TabNet:\n",
    "    def __init__(self, ratings, relevance, seed=42,width_values = 8, steps = 3, learning_rate = 2e-2):\n",
    "        self.aug = RegressionSMOTE(p=0.2)\n",
    "        #! df['rating'] = df['rating'].astype('float16')\n",
    "\n",
    "        # Reduce genome-score size\n",
    "\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "\n",
    "        # Pivot the relevance DataFrame to create a matrix of tag relevance scores for each movie\n",
    "        relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "        # Merge the ratings and relevance data\n",
    "        mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "        X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "        #X = addColumnOperation(ratings,X)\n",
    "        X.columns = X.columns.astype(str)\n",
    "        ratings = None  \n",
    "        train = X\n",
    "        # mescolare le righe del DataFrame\n",
    "        X = X.sample(frac=1,random_state = seed).reset_index(drop=True)\n",
    "        \n",
    "        if \"Set\" not in train.columns:\n",
    "            train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n",
    "\n",
    "        features = [ col for col in train.columns if col not in [\"rating\", \"Set\"]]\n",
    "        target = \"rating\"\n",
    "        \n",
    "        train_indices = train[train.Set==\"train\"].index\n",
    "        valid_indices = train[train.Set==\"valid\"].index\n",
    "        test_indices = train[train.Set==\"test\"].index\n",
    "\n",
    "        self.X_train = train[features].values[train_indices]\n",
    "        self.y_train = train[target].values[train_indices].reshape(-1, 1)\n",
    "\n",
    "        self.X_valid = train[features].values[valid_indices]\n",
    "        self.y_valid = train[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "        self.X_test = train[features].values[test_indices]\n",
    "        self.y_test = train[target].values[test_indices].reshape(-1, 1)\n",
    "\n",
    "        # pca = PCA()\n",
    "        # pca.fit(self.X_train)\n",
    "        # self.X_train = pca.transform(self.X_train)\n",
    "        # self.X_test = pca.transform(self.X_test)\n",
    "        # self.X_valid = pca.transform(self.X_valid)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.model = TabNetRegressor(n_d = width_values, \n",
    "                                         n_a = width_values , \n",
    "                                         n_steps = steps, \n",
    "                                         optimizer_params = \n",
    "                                         dict(lr=learning_rate), \n",
    "                                         seed=seed,  \n",
    "                                         verbose=0, \n",
    "                                         device_name=\"cuda\")  \n",
    "        else:\n",
    "            self.model = TabNetRegressor(n_d = width_values, n_a = width_values , n_steps = steps, optimizer_params = dict(lr=learning_rate), seed=seed, verbose=0)  \n",
    "\n",
    "\n",
    "\n",
    "    def train(self,max_epochs = 150,batchsize = 1024):\n",
    "        self.model.fit(\n",
    "            X_train=self.X_train, y_train=self.y_train,\n",
    "            eval_set=[(self.X_train,self.y_train), (self.X_valid, self.y_valid)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "            max_epochs=max_epochs,\n",
    "            patience=20,\n",
    "            batch_size=batchsize, virtual_batch_size=1024,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            augmentations=self.aug, #aug,\n",
    "        ) \n",
    "\n",
    "        return self.model.history\n",
    "\n",
    "    def test(self):\n",
    "        # Predict the labels of the test set: y_pred\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "\n",
    "        # Compute the mean squared error\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        rmse = mean_squared_error(self.y_test, y_pred, squared=False)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"MSE: {mse} RMSE: {rmse} R2: {r2} MAE: {mae}\")\n",
    "        print(\"=====================================\")\n",
    "        return r2,self.model\n",
    "    \n",
    "    def load(self,model):\n",
    "        self.model =TabNetRegressor()\n",
    "        self.model.load_model(model)\n",
    "    \n",
    "    def save(self,root,name):\n",
    "        self.model.save_model(f\"{root}/{name}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabNet(ratings, genome_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.00898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008375112472055 RMSE: 0.09151564058703299 R2: 0.9629266041726134 MAE: 0.07119005210035172\n",
      "=====================================\n",
      "Successfully saved model at model/512_8_3_0.02_70.pt.zip\n",
      "New best model: 512_8_3_0.02_70 with r2: 0.9629266041726134\n",
      "[2/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 112 and best_valid_mse = 0.00727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007141037128486071 RMSE: 0.08450465743665299 R2: 0.9683893802064406 MAE: 0.064916241985196\n",
      "=====================================\n",
      "Successfully saved model at model/512_8_3_0.02_120.pt.zip\n",
      "New best model: 512_8_3_0.02_120 with r2: 0.9683893802064406\n",
      "[3/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 112 and best_valid_mse = 0.00727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007141037128486071 RMSE: 0.08450465743665299 R2: 0.9683893802064406 MAE: 0.064916241985196\n",
      "=====================================\n",
      "[4/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 112 and best_valid_mse = 0.00727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007141037128486071 RMSE: 0.08450465743665299 R2: 0.9683893802064406 MAE: 0.064916241985196\n",
      "=====================================\n",
      "[5/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 66 and best_valid_mse = 0.01092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011819705597407698 RMSE: 0.1087184694401448 R2: 0.9476787176723906 MAE: 0.08514873473634856\n",
      "=====================================\n",
      "[6/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 116 and best_valid_mse = 0.00795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008138680034445466 RMSE: 0.09021463315031251 R2: 0.963973199472101 MAE: 0.06987249606534694\n",
      "=====================================\n",
      "[7/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 126 and best_valid_mse = 0.00778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007943909841135579 RMSE: 0.08912861404249244 R2: 0.9648353720693106 MAE: 0.06908251466365427\n",
      "=====================================\n",
      "[8/216] START => batchsize: 512 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 126 and best_valid_mse = 0.00778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007943909841135579 RMSE: 0.08912861404249244 R2: 0.9648353720693106 MAE: 0.06908251466365427\n",
      "=====================================\n",
      "[9/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.00818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008558012322190566 RMSE: 0.09250952557542691 R2: 0.9621169770107729 MAE: 0.07158280800057475\n",
      "=====================================\n",
      "[10/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007134889267118613 RMSE: 0.08446827373113892 R2: 0.9684165944198292 MAE: 0.06500316417632032\n",
      "=====================================\n",
      "Successfully saved model at model/512_8_5_0.02_120.pt.zip\n",
      "New best model: 512_8_5_0.02_120 with r2: 0.9684165944198292\n",
      "[11/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 133 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007134889267118613 RMSE: 0.08446827373113892 R2: 0.9684165944198292 MAE: 0.06500316417632032\n",
      "=====================================\n",
      "[12/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 133 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007134889267118613 RMSE: 0.08446827373113892 R2: 0.9684165944198292 MAE: 0.06500316417632032\n",
      "=====================================\n",
      "[13/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 63 and best_valid_mse = 0.01412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016109342887157978 RMSE: 0.12692258619787883 R2: 0.9286901462675927 MAE: 0.0972695573526162\n",
      "=====================================\n",
      "[14/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 115 and best_valid_mse = 0.00798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009108853428546749 RMSE: 0.09544031343487273 R2: 0.9596786156822441 MAE: 0.07317697138838591\n",
      "=====================================\n",
      "[15/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 140 and best_valid_mse = 0.00717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00822872809439037 RMSE: 0.09071233705726234 R2: 0.963574591407915 MAE: 0.06924833909929502\n",
      "=====================================\n",
      "[16/216] START => batchsize: 512 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 160 with best_epoch = 140 and best_valid_mse = 0.00717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00822872809439037 RMSE: 0.09071233705726234 R2: 0.963574591407915 MAE: 0.06924833909929502\n",
      "=====================================\n",
      "[17/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.01289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.015866123772218528 RMSE: 0.1259608025229219 R2: 0.929766783572587 MAE: 0.09725992859282552\n",
      "=====================================\n",
      "[18/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 93 and best_valid_mse = 0.00834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009278571397335723 RMSE: 0.0963253414078337 R2: 0.9589273396299012 MAE: 0.07514834659452889\n",
      "=====================================\n",
      "[19/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 93 and best_valid_mse = 0.00834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009278571397335723 RMSE: 0.0963253414078337 R2: 0.9589273396299012 MAE: 0.07514834659452889\n",
      "=====================================\n",
      "[20/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 93 and best_valid_mse = 0.00834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009278571397335723 RMSE: 0.0963253414078337 R2: 0.9589273396299012 MAE: 0.07514834659452889\n",
      "=====================================\n",
      "[21/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.02622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02663869960772536 RMSE: 0.16321366244198235 R2: 0.8820807412223653 MAE: 0.12632981284162598\n",
      "=====================================\n",
      "[22/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 104 and best_valid_mse = 0.01322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013623766295144592 RMSE: 0.11672089056867495 R2: 0.9396928361016063 MAE: 0.09133227029187603\n",
      "=====================================\n",
      "[23/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 124 with best_epoch = 104 and best_valid_mse = 0.01322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013623766295144592 RMSE: 0.11672089056867495 R2: 0.9396928361016063 MAE: 0.09133227029187603\n",
      "=====================================\n",
      "[24/216] START => batchsize: 512 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 124 with best_epoch = 104 and best_valid_mse = 0.01322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013623766295144592 RMSE: 0.11672089056867495 R2: 0.9396928361016063 MAE: 0.09133227029187603\n",
      "=====================================\n",
      "[25/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 55 and best_valid_mse = 0.00805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008647970421262686 RMSE: 0.09299446446570186 R2: 0.9617187671687071 MAE: 0.0723402907145758\n",
      "=====================================\n",
      "[26/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_valid_mse = 0.00805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008647970421262686 RMSE: 0.09299446446570186 R2: 0.9617187671687071 MAE: 0.0723402907145758\n",
      "=====================================\n",
      "[27/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_valid_mse = 0.00805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008647970421262686 RMSE: 0.09299446446570186 R2: 0.9617187671687071 MAE: 0.0723402907145758\n",
      "=====================================\n",
      "[28/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 55 and best_valid_mse = 0.00805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008647970421262686 RMSE: 0.09299446446570186 R2: 0.9617187671687071 MAE: 0.0723402907145758\n",
      "=====================================\n",
      "[29/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.01055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012659221029222591 RMSE: 0.11251320379947675 R2: 0.9439625063366358 MAE: 0.08604329448006395\n",
      "=====================================\n",
      "[30/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 106 and best_valid_mse = 0.00791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009565823222873627 RMSE: 0.09780502657263392 R2: 0.9576557864817088 MAE: 0.07502581652147858\n",
      "=====================================\n",
      "[31/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 126 with best_epoch = 106 and best_valid_mse = 0.00791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009565823222873627 RMSE: 0.09780502657263392 R2: 0.9576557864817088 MAE: 0.07502581652147858\n",
      "=====================================\n",
      "[32/216] START => batchsize: 512 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 126 with best_epoch = 106 and best_valid_mse = 0.00791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009565823222873627 RMSE: 0.09780502657263392 R2: 0.9576557864817088 MAE: 0.07502581652147858\n",
      "=====================================\n",
      "[33/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.01143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011728217246775918 RMSE: 0.10829689398489653 R2: 0.9480837013484766 MAE: 0.08467057999171625\n",
      "=====================================\n",
      "[34/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007549762483588407 RMSE: 0.08688936922079943 R2: 0.9665801105488738 MAE: 0.06660373809016729\n",
      "=====================================\n",
      "[35/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 133 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007549762483588407 RMSE: 0.08688936922079943 R2: 0.9665801105488738 MAE: 0.06660373809016729\n",
      "=====================================\n",
      "[36/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 133 with best_epoch = 113 and best_valid_mse = 0.00673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007549762483588407 RMSE: 0.08688936922079943 R2: 0.9665801105488738 MAE: 0.06660373809016729\n",
      "=====================================\n",
      "[37/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.02021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02198564776118744 RMSE: 0.14827558046147532 R2: 0.9026780088396831 MAE: 0.11326929337090523\n",
      "=====================================\n",
      "[38/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.01676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01735769241558445 RMSE: 0.13174859549757809 R2: 0.9231641839175099 MAE: 0.1017932363163558\n",
      "=====================================\n",
      "[39/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.01676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01735769241558445 RMSE: 0.13174859549757809 R2: 0.9231641839175099 MAE: 0.1017932363163558\n",
      "=====================================\n",
      "[40/216] START => batchsize: 512 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.01676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01735769241558445 RMSE: 0.13174859549757809 R2: 0.9231641839175099 MAE: 0.1017932363163558\n",
      "=====================================\n",
      "[41/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.01682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.017634763614599785 RMSE: 0.1327959472822864 R2: 0.9219376964801484 MAE: 0.1005154764386585\n",
      "=====================================\n",
      "[42/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.00858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008656045050909248 RMSE: 0.09303786890782295 R2: 0.9616830238945666 MAE: 0.07288296679932475\n",
      "=====================================\n",
      "[43/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 144 with best_epoch = 124 and best_valid_mse = 0.00721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0073010252065119855 RMSE: 0.08544603680986021 R2: 0.9676811746313981 MAE: 0.06584019803249924\n",
      "=====================================\n",
      "[44/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 144 with best_epoch = 124 and best_valid_mse = 0.00721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0073010252065119855 RMSE: 0.08544603680986021 R2: 0.9676811746313981 MAE: 0.06584019803249924\n",
      "=====================================\n",
      "[45/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.02767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.028170894250910218 RMSE: 0.16784187275799273 R2: 0.8752983059200421 MAE: 0.12942559460707914\n",
      "=====================================\n",
      "[46/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 116 and best_valid_mse = 0.00985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010791597660126176 RMSE: 0.10388261481174882 R2: 0.9522297553616509 MAE: 0.07900345731809026\n",
      "=====================================\n",
      "[47/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.00794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008551167299655413 RMSE: 0.09247252186274263 R2: 0.9621472772880216 MAE: 0.07111861183812761\n",
      "=====================================\n",
      "[48/216] START => batchsize: 512 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 194 with best_epoch = 174 and best_valid_mse = 0.00703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007767138002495911 RMSE: 0.08813136786919804 R2: 0.9656178728854955 MAE: 0.06724462350560771\n",
      "=====================================\n",
      "[49/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 57 and best_valid_mse = 0.00812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008583657804481162 RMSE: 0.09264803184353762 R2: 0.9620034543423531 MAE: 0.07248430750330484\n",
      "=====================================\n",
      "[50/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.00706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007626782981279661 RMSE: 0.08733145470722253 R2: 0.9662391704830231 MAE: 0.0669989335259039\n",
      "=====================================\n",
      "[51/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.00706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007626782981279661 RMSE: 0.08733145470722253 R2: 0.9662391704830231 MAE: 0.0669989335259039\n",
      "=====================================\n",
      "[52/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 74 and best_valid_mse = 0.00706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007626782981279661 RMSE: 0.08733145470722253 R2: 0.9662391704830231 MAE: 0.0669989335259039\n",
      "=====================================\n",
      "[53/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.01398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.014058218174627652 RMSE: 0.11856735712086885 R2: 0.9377696850335137 MAE: 0.09152624288609543\n",
      "=====================================\n",
      "[54/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 114 and best_valid_mse = 0.00854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008434985775528236 RMSE: 0.09184217863012743 R2: 0.9626615681284334 MAE: 0.07048662990423411\n",
      "=====================================\n",
      "[55/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 114 and best_valid_mse = 0.00854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008434985775528236 RMSE: 0.09184217863012743 R2: 0.9626615681284334 MAE: 0.07048662990423411\n",
      "=====================================\n",
      "[56/216] START => batchsize: 512 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 114 and best_valid_mse = 0.00854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008434985775528236 RMSE: 0.09184217863012743 R2: 0.9626615681284334 MAE: 0.07048662990423411\n",
      "=====================================\n",
      "[57/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 60 and best_valid_mse = 0.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010189981961128572 RMSE: 0.10094544051678893 R2: 0.9548928762474096 MAE: 0.07836096113731149\n",
      "=====================================\n",
      "[58/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 84 and best_valid_mse = 0.00766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008717751900092137 RMSE: 0.093368902211026 R2: 0.9614098714500291 MAE: 0.0719504365574335\n",
      "=====================================\n",
      "[59/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 84 and best_valid_mse = 0.00766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008717751900092137 RMSE: 0.093368902211026 R2: 0.9614098714500291 MAE: 0.0719504365574335\n",
      "=====================================\n",
      "[60/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 84 and best_valid_mse = 0.00766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008717751900092137 RMSE: 0.093368902211026 R2: 0.9614098714500291 MAE: 0.0719504365574335\n",
      "=====================================\n",
      "[61/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.01846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.018030326921231014 RMSE: 0.13427705284683236 R2: 0.9201866901395815 MAE: 0.10480189396302236\n",
      "=====================================\n",
      "[62/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 110 and best_valid_mse = 0.00962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010317578492272595 RMSE: 0.10157548174767667 R2: 0.9543280555693485 MAE: 0.07908120993278969\n",
      "=====================================\n",
      "[63/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 142 and best_valid_mse = 0.00826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008657098758644984 RMSE: 0.09304353152500706 R2: 0.9616783595364337 MAE: 0.07135108432468051\n",
      "=====================================\n",
      "[64/216] START => batchsize: 512 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 201 with best_epoch = 181 and best_valid_mse = 0.00642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006612335665050214 RMSE: 0.08131626937489332 R2: 0.9707297378665768 MAE: 0.06346850942509422\n",
      "=====================================\n",
      "Successfully saved model at model/512_32_5_0.01_210.pt.zip\n",
      "New best model: 512_32_5_0.01_210 with r2: 0.9707297378665768\n",
      "[65/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 53 and best_valid_mse = 0.01089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010311334675286728 RMSE: 0.10154474223359242 R2: 0.9543556945412863 MAE: 0.07957523635350915\n",
      "=====================================\n",
      "[66/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_valid_mse = 0.01089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010311334675286728 RMSE: 0.10154474223359242 R2: 0.9543556945412863 MAE: 0.07957523635350915\n",
      "=====================================\n",
      "[67/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_valid_mse = 0.01089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010311334675286728 RMSE: 0.10154474223359242 R2: 0.9543556945412863 MAE: 0.07957523635350915\n",
      "=====================================\n",
      "[68/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_valid_mse = 0.01089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010311334675286728 RMSE: 0.10154474223359242 R2: 0.9543556945412863 MAE: 0.07957523635350915\n",
      "=====================================\n",
      "[69/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.02821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02852075762547459 RMSE: 0.1688808977518612 R2: 0.8737495955697026 MAE: 0.13185311188403784\n",
      "=====================================\n",
      "[70/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 106 and best_valid_mse = 0.01289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01265242483558254 RMSE: 0.1124829979845067 R2: 0.9439925904671806 MAE: 0.08714293149984562\n",
      "=====================================\n",
      "[71/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010527390654537743 RMSE: 0.10260307331916399 R2: 0.953399297971524 MAE: 0.07461142456596287\n",
      "=====================================\n",
      "[72/216] START => batchsize: 512 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 181 with best_epoch = 161 and best_valid_mse = 0.00746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007798787875814569 RMSE: 0.08831074609476793 R2: 0.9654777710915978 MAE: 0.06845275130375078\n",
      "=====================================\n",
      "[73/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.00984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011622210137648777 RMSE: 0.10780635481106286 R2: 0.9485529539740744 MAE: 0.07918314045083846\n",
      "=====================================\n",
      "[74/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 105 and best_valid_mse = 0.00867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009875612527464245 RMSE: 0.09937611648411425 R2: 0.9562844685978594 MAE: 0.07677445818204054\n",
      "=====================================\n",
      "[75/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 122 and best_valid_mse = 0.00849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009241560466106897 RMSE: 0.0961330352485913 R2: 0.9590911727614518 MAE: 0.07373088380949672\n",
      "=====================================\n",
      "[76/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 122 and best_valid_mse = 0.00849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009241560466106897 RMSE: 0.0961330352485913 R2: 0.9590911727614518 MAE: 0.07373088380949672\n",
      "=====================================\n",
      "[77/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.017687417889978966 RMSE: 0.1329940520849672 R2: 0.9217046162917152 MAE: 0.10268828132515247\n",
      "=====================================\n",
      "[78/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 113 and best_valid_mse = 0.01098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012074888754559787 RMSE: 0.10988579869373379 R2: 0.9465491201624887 MAE: 0.0854801873071521\n",
      "=====================================\n",
      "[79/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_valid_mse = 0.00933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010154190684123042 RMSE: 0.10076800426783812 R2: 0.9550513104396693 MAE: 0.07840252072567427\n",
      "=====================================\n",
      "[80/216] START => batchsize: 1024 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 200 and best_valid_mse = 0.00781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009122723252845999 RMSE: 0.09551294809001551 R2: 0.9596172193143722 MAE: 0.07356143214590999\n",
      "=====================================\n",
      "[81/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 57 and best_valid_mse = 0.02142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021867342091138563 RMSE: 0.14787610385433667 R2: 0.9032017024556173 MAE: 0.11562706844187325\n",
      "=====================================\n",
      "[82/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010789114385916003 RMSE: 0.10387066181514394 R2: 0.9522407478597277 MAE: 0.08168654386534051\n",
      "=====================================\n",
      "[83/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.00766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008824375481557381 RMSE: 0.09393814710519567 R2: 0.9609378899389289 MAE: 0.07310606435466165\n",
      "=====================================\n",
      "[84/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 180 with best_epoch = 160 and best_valid_mse = 0.00713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008503297348262814 RMSE: 0.09221332522072292 R2: 0.9623591791176552 MAE: 0.07136311903686834\n",
      "=====================================\n",
      "[85/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.02467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.026391994900956065 RMSE: 0.16245613223561634 R2: 0.8831728078993271 MAE: 0.12486284737274042\n",
      "=====================================\n",
      "[86/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 76 and best_valid_mse = 0.02246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.025655173469957767 RMSE: 0.1601723242946726 R2: 0.8864344324633728 MAE: 0.12301658799611564\n",
      "=====================================\n",
      "[87/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 76 and best_valid_mse = 0.02246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.025655173469957767 RMSE: 0.1601723242946726 R2: 0.8864344324633728 MAE: 0.12301658799611564\n",
      "=====================================\n",
      "[88/216] START => batchsize: 1024 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 76 and best_valid_mse = 0.02246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.025655173469957767 RMSE: 0.1601723242946726 R2: 0.8864344324633728 MAE: 0.12301658799611564\n",
      "=====================================\n",
      "[89/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 64 and best_valid_mse = 0.03534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03440064090789094 RMSE: 0.1854740976737478 R2: 0.8477216179066913 MAE: 0.14315428522491794\n",
      "=====================================\n",
      "[90/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.01119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01170717887502426 RMSE: 0.108199717536712 R2: 0.9481768301137459 MAE: 0.08445318955064202\n",
      "=====================================\n",
      "[91/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_valid_mse = 0.00876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009789056496205605 RMSE: 0.09893966088584297 R2: 0.9566676187965949 MAE: 0.07639472616605016\n",
      "=====================================\n",
      "[92/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 153 with best_epoch = 133 and best_valid_mse = 0.00876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009789056496205605 RMSE: 0.09893966088584297 R2: 0.9566676187965949 MAE: 0.07639472616605016\n",
      "=====================================\n",
      "[93/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.03444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03402042042020673 RMSE: 0.18444625347294732 R2: 0.8494047074996529 MAE: 0.14220069162230795\n",
      "=====================================\n",
      "[94/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 102 and best_valid_mse = 0.02836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.028690767318405874 RMSE: 0.1693834918709786 R2: 0.8729970281669883 MAE: 0.13000789111154232\n",
      "=====================================\n",
      "[95/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_valid_mse = 0.02303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021756850272302036 RMSE: 0.14750203480732743 R2: 0.9036908071630565 MAE: 0.11580096166199103\n",
      "=====================================\n",
      "[96/216] START => batchsize: 1024 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 206 and best_valid_mse = 0.01456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.014261643606721613 RMSE: 0.11942212360664842 R2: 0.9368691990292313 MAE: 0.09184155490074283\n",
      "=====================================\n",
      "[97/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.01163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012593156980683579 RMSE: 0.11221923623284726 R2: 0.9442549464238129 MAE: 0.08673665014063123\n",
      "=====================================\n",
      "[98/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_valid_mse = 0.00951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010208465452036438 RMSE: 0.10103695092408736 R2: 0.9548110569551929 MAE: 0.07708228677870457\n",
      "=====================================\n",
      "[99/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_valid_mse = 0.00951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010208465452036438 RMSE: 0.10103695092408736 R2: 0.9548110569551929 MAE: 0.07708228677870457\n",
      "=====================================\n",
      "[100/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 85 and best_valid_mse = 0.00951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010208465452036438 RMSE: 0.10103695092408736 R2: 0.9548110569551929 MAE: 0.07708228677870457\n",
      "=====================================\n",
      "[101/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.02407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02361152188380732 RMSE: 0.15366041091903704 R2: 0.8954808905783445 MAE: 0.1199640090960818\n",
      "=====================================\n",
      "[102/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 117 and best_valid_mse = 0.01382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013986024633472656 RMSE: 0.11826252421402418 R2: 0.9380892580226945 MAE: 0.09256322246229669\n",
      "=====================================\n",
      "[103/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_valid_mse = 0.01183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012684101958267673 RMSE: 0.1126237184533865 R2: 0.9438523680508373 MAE: 0.0872954752981479\n",
      "=====================================\n",
      "[104/216] START => batchsize: 1024 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 190 and best_valid_mse = 0.00973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010819029369025276 RMSE: 0.1040145632545043 R2: 0.9521083257563016 MAE: 0.0799307262576551\n",
      "=====================================\n",
      "[105/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.03453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.037162012855821436 RMSE: 0.19277451298297044 R2: 0.8354980883011063 MAE: 0.14586767394588587\n",
      "=====================================\n",
      "[106/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 117 and best_valid_mse = 0.01198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012229149411796926 RMSE: 0.11058548463427254 R2: 0.9458662676723966 MAE: 0.0862227680042081\n",
      "=====================================\n",
      "[107/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_valid_mse = 0.01198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012229149411796926 RMSE: 0.11058548463427254 R2: 0.9458662676723966 MAE: 0.0862227680042081\n",
      "=====================================\n",
      "[108/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_valid_mse = 0.01198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012229149411796926 RMSE: 0.11058548463427254 R2: 0.9458662676723966 MAE: 0.0862227680042081\n",
      "=====================================\n",
      "[109/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.02569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02848147850136505 RMSE: 0.16876456530138384 R2: 0.8739234691171593 MAE: 0.12919991373026074\n",
      "=====================================\n",
      "[110/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 102 and best_valid_mse = 0.02065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021383570062010113 RMSE: 0.14623122122860807 R2: 0.9053431748222202 MAE: 0.11478096877611922\n",
      "=====================================\n",
      "[111/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_mse = 0.02065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021383570062010113 RMSE: 0.14623122122860807 R2: 0.9053431748222202 MAE: 0.11478096877611922\n",
      "=====================================\n",
      "[112/216] START => batchsize: 1024 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_mse = 0.02065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021383570062010113 RMSE: 0.14623122122860807 R2: 0.9053431748222202 MAE: 0.11478096877611922\n",
      "=====================================\n",
      "[113/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.02871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02913117571377433 RMSE: 0.1706785742668784 R2: 0.8710475098982273 MAE: 0.13315613724781766\n",
      "=====================================\n",
      "[114/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 112 and best_valid_mse = 0.02184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02205696210976013 RMSE: 0.14851586484197615 R2: 0.902362327697295 MAE: 0.1172270965919984\n",
      "=====================================\n",
      "[115/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 112 and best_valid_mse = 0.02184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02205696210976013 RMSE: 0.14851586484197615 R2: 0.902362327697295 MAE: 0.1172270965919984\n",
      "=====================================\n",
      "[116/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 112 and best_valid_mse = 0.02184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02205696210976013 RMSE: 0.14851586484197615 R2: 0.902362327697295 MAE: 0.1172270965919984\n",
      "=====================================\n",
      "[117/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 60 and best_valid_mse = 0.03428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0350619874534745 RMSE: 0.18724846448896315 R2: 0.8447940915785013 MAE: 0.14483013092895322\n",
      "=====================================\n",
      "[118/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 113 and best_valid_mse = 0.02579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.023830028113339525 RMSE: 0.15436977720181994 R2: 0.8945136476947143 MAE: 0.1215037776431553\n",
      "=====================================\n",
      "[119/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_valid_mse = 0.02196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.022046572683668587 RMSE: 0.14848088322632172 R2: 0.9024083176833634 MAE: 0.11671216213316794\n",
      "=====================================\n",
      "[120/216] START => batchsize: 1024 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 209 and best_valid_mse = 0.01399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.014118470005917231 RMSE: 0.11882116817266708 R2: 0.9375029733925445 MAE: 0.09390490996478155\n",
      "=====================================\n",
      "[121/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.01168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013231213670586344 RMSE: 0.11502701278650308 R2: 0.9414305153127078 MAE: 0.08837258046907047\n",
      "=====================================\n",
      "[122/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 88 and best_valid_mse = 0.00983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010982032237949984 RMSE: 0.10479519186465562 R2: 0.951386774863605 MAE: 0.08012447807147817\n",
      "=====================================\n",
      "[123/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 88 and best_valid_mse = 0.00983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010982032237949984 RMSE: 0.10479519186465562 R2: 0.951386774863605 MAE: 0.08012447807147817\n",
      "=====================================\n",
      "[124/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 88 and best_valid_mse = 0.00983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010982032237949984 RMSE: 0.10479519186465562 R2: 0.951386774863605 MAE: 0.08012447807147817\n",
      "=====================================\n",
      "[125/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.01484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016797607113182872 RMSE: 0.12960558287814175 R2: 0.9256434657399711 MAE: 0.09753317090168223\n",
      "=====================================\n",
      "[126/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 114 and best_valid_mse = 0.01076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.012555736827355873 RMSE: 0.11205238430018288 R2: 0.9444205910239145 MAE: 0.08400504967873651\n",
      "=====================================\n",
      "[127/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_valid_mse = 0.01007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011144089059904045 RMSE: 0.10556556758670908 R2: 0.950669411756318 MAE: 0.0793923232805667\n",
      "=====================================\n",
      "[128/216] START => batchsize: 1024 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 153 with best_epoch = 133 and best_valid_mse = 0.01007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011144089059904045 RMSE: 0.10556556758670908 R2: 0.950669411756318 MAE: 0.0793923232805667\n",
      "=====================================\n",
      "[129/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.02445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.022304647797904475 RMSE: 0.1493474063983184 R2: 0.9012659185937762 MAE: 0.11562770358514915\n",
      "=====================================\n",
      "[130/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.00956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009894927898657313 RMSE: 0.09947325217694108 R2: 0.9561989668921591 MAE: 0.07682242366835725\n",
      "=====================================\n",
      "[131/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 133 and best_valid_mse = 0.00858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008609746380974316 RMSE: 0.09278871903940865 R2: 0.961887970266631 MAE: 0.07195077009615775\n",
      "=====================================\n",
      "[132/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 153 with best_epoch = 133 and best_valid_mse = 0.00858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008609746380974316 RMSE: 0.09278871903940865 R2: 0.961887970266631 MAE: 0.07195077009615775\n",
      "=====================================\n",
      "[133/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 64 and best_valid_mse = 0.02459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.023601290263645564 RMSE: 0.1536271143504478 R2: 0.8955261820183673 MAE: 0.12131727959445024\n",
      "=====================================\n",
      "[134/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.01847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02062917473229941 RMSE: 0.14362859998029434 R2: 0.9086825922643162 MAE: 0.10969209936711113\n",
      "=====================================\n",
      "[135/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 145 with best_epoch = 125 and best_valid_mse = 0.01738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01935961132484711 RMSE: 0.13913882033727004 R2: 0.9143024602827449 MAE: 0.10583662678445929\n",
      "=====================================\n",
      "[136/216] START => batchsize: 1024 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 145 with best_epoch = 125 and best_valid_mse = 0.01738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01935961132484711 RMSE: 0.13913882033727004 R2: 0.9143024602827449 MAE: 0.10583662678445929\n",
      "=====================================\n",
      "[137/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.03578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03694351132351565 RMSE: 0.19220694920713885 R2: 0.836465310391925 MAE: 0.14850222205758573\n",
      "=====================================\n",
      "[138/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 82 and best_valid_mse = 0.02811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.026471739716628844 RMSE: 0.1627013820366282 R2: 0.8828198083275024 MAE: 0.12606350207318956\n",
      "=====================================\n",
      "[139/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 82 and best_valid_mse = 0.02811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.026471739716628844 RMSE: 0.1627013820366282 R2: 0.8828198083275024 MAE: 0.12606350207318956\n",
      "=====================================\n",
      "[140/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 82 and best_valid_mse = 0.02811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.026471739716628844 RMSE: 0.1627013820366282 R2: 0.8828198083275024 MAE: 0.12606350207318956\n",
      "=====================================\n",
      "[141/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_mse = 0.03478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.037760669213400995 RMSE: 0.19432104675871062 R2: 0.832848067279516 MAE: 0.1496782424967345\n",
      "=====================================\n",
      "[142/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_mse = 0.03478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.037760669213400995 RMSE: 0.19432104675871062 R2: 0.832848067279516 MAE: 0.1496782424967345\n",
      "=====================================\n",
      "[143/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_mse = 0.03478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.037760669213400995 RMSE: 0.19432104675871062 R2: 0.832848067279516 MAE: 0.1496782424967345\n",
      "=====================================\n",
      "[144/216] START => batchsize: 1024 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 68 with best_epoch = 48 and best_valid_mse = 0.03478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.037760669213400995 RMSE: 0.19432104675871062 R2: 0.832848067279516 MAE: 0.1496782424967345\n",
      "=====================================\n",
      "[145/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02395427708844633 RMSE: 0.15477169343405897 R2: 0.8939636453573545 MAE: 0.12396044094449743\n",
      "=====================================\n",
      "[146/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 117 and best_valid_mse = 0.00872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00957516995897943 RMSE: 0.09785279739986706 R2: 0.9576144120824395 MAE: 0.07598936641230457\n",
      "=====================================\n",
      "[147/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_valid_mse = 0.00793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008334083716089211 RMSE: 0.09129120284063087 R2: 0.9631082226661323 MAE: 0.07102421502504455\n",
      "=====================================\n",
      "[148/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 164 with best_epoch = 144 and best_valid_mse = 0.00793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.008334083716089211 RMSE: 0.09129120284063087 R2: 0.9631082226661323 MAE: 0.07102421502504455\n",
      "=====================================\n",
      "[149/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.03724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03650514800004029 RMSE: 0.19106320420227513 R2: 0.8384057759154193 MAE: 0.15371554649534505\n",
      "=====================================\n",
      "[150/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.01638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016872618587778145 RMSE: 0.12989464418434712 R2: 0.9253114188452524 MAE: 0.0995969313481082\n",
      "=====================================\n",
      "[151/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.01407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013706762240151207 RMSE: 0.117075882401762 R2: 0.9393254450329411 MAE: 0.08922299251962083\n",
      "=====================================\n",
      "[152/216] START => batchsize: 2048 width: 8 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 200 and best_valid_mse = 0.01056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011769814717027524 RMSE: 0.10848877691737299 R2: 0.9478995653759508 MAE: 0.08129944172682498\n",
      "=====================================\n",
      "[153/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.04491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04519727456621285 RMSE: 0.21259650647697118 R2: 0.7999290808447933 MAE: 0.1698057028276727\n",
      "=====================================\n",
      "[154/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.01715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016594637691366033 RMSE: 0.1288201757931033 R2: 0.926541933162466 MAE: 0.10031240820390447\n",
      "=====================================\n",
      "[155/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 139 and best_valid_mse = 0.01427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013711857077734643 RMSE: 0.11709763907839749 R2: 0.9393028921500949 MAE: 0.0909214038412098\n",
      "=====================================\n",
      "[156/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 206 and best_valid_mse = 0.00813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.007957497740721953 RMSE: 0.08920480783411819 R2: 0.9647752236735118 MAE: 0.06890399511740031\n",
      "=====================================\n",
      "[157/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.03883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04000648991332048 RMSE: 0.2000162241252456 R2: 0.8229066843974037 MAE: 0.15371655535763756\n",
      "=====================================\n",
      "[158/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 117 and best_valid_mse = 0.02651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.028205834084658526 RMSE: 0.16794592607341963 R2: 0.8751436407390053 MAE: 0.13143553825550452\n",
      "=====================================\n",
      "[159/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.02405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.025554110439573882 RMSE: 0.15985653080050838 R2: 0.8868817995574195 MAE: 0.12634906903847543\n",
      "=====================================\n",
      "[160/216] START => batchsize: 2048 width: 8 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 208 and best_valid_mse = 0.01851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.019784943211064814 RMSE: 0.14065896064973896 R2: 0.9124196799107352 MAE: 0.10967189132995135\n",
      "=====================================\n",
      "[161/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 65 and best_valid_mse = 0.07879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07697506760292173 RMSE: 0.27744380981186395 R2: 0.659261035645204 MAE: 0.22481702105413884\n",
      "=====================================\n",
      "[162/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 115 and best_valid_mse = 0.03364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03305079673856158 RMSE: 0.1817987809050478 R2: 0.853696857924281 MAE: 0.13981453257582094\n",
      "=====================================\n",
      "[163/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.03088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.029799521386948424 RMSE: 0.17262537874527148 R2: 0.8680890011290878 MAE: 0.13331572041303533\n",
      "=====================================\n",
      "[164/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 208 and best_valid_mse = 0.01732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01942429719579797 RMSE: 0.13937107732882734 R2: 0.914016120856713 MAE: 0.10758297533462027\n",
      "=====================================\n",
      "[165/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.06476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.06106956504096897 RMSE: 0.2471225708853179 R2: 0.7296685668013914 MAE: 0.19937506264368068\n",
      "=====================================\n",
      "[166/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.03234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.029980566639710635 RMSE: 0.17314897238999322 R2: 0.8672875835552083 MAE: 0.13201246305590936\n",
      "=====================================\n",
      "[167/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_valid_mse = 0.03098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.029001326511516767 RMSE: 0.1702977583866469 R2: 0.8716223022833121 MAE: 0.12868894609230358\n",
      "=====================================\n",
      "[168/216] START => batchsize: 2048 width: 8 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 157 with best_epoch = 137 and best_valid_mse = 0.03098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.029001326511516767 RMSE: 0.1702977583866469 R2: 0.8716223022833121 MAE: 0.12868894609230358\n",
      "=====================================\n",
      "[169/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.02329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02224361730693282 RMSE: 0.14914294253142796 R2: 0.901536077061126 MAE: 0.12033596044558056\n",
      "=====================================\n",
      "[170/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.00995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011041300761627156 RMSE: 0.10507759400379872 R2: 0.9511244159465492 MAE: 0.08081405903659018\n",
      "=====================================\n",
      "[171/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 137 and best_valid_mse = 0.00922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.010153319674260992 RMSE: 0.10076368231789165 R2: 0.9550551660647123 MAE: 0.07856697110268633\n",
      "=====================================\n",
      "[172/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 191 and best_valid_mse = 0.00805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.009007841415351677 RMSE: 0.09490964869470163 R2: 0.9601257569428537 MAE: 0.07359046048737412\n",
      "=====================================\n",
      "[173/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.03416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.036600084044667136 RMSE: 0.1913114843512201 R2: 0.8379855306264777 MAE: 0.15426421877231924\n",
      "=====================================\n",
      "[174/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 115 and best_valid_mse = 0.01849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01948787676802211 RMSE: 0.13959898555513256 R2: 0.9137346785888669 MAE: 0.10766097020677794\n",
      "=====================================\n",
      "[175/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_valid_mse = 0.01634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.016572914149714464 RMSE: 0.12873583086970955 R2: 0.9266380949108733 MAE: 0.09881818494559975\n",
      "=====================================\n",
      "[176/216] START => batchsize: 2048 width: 16 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 198 and best_valid_mse = 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.014777682321918333 RMSE: 0.12156349090873597 R2: 0.9345848944763576 MAE: 0.09301762578823235\n",
      "=====================================\n",
      "[177/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.04047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03802132985724896 RMSE: 0.19499058915047401 R2: 0.8316942230465888 MAE: 0.15066217910318802\n",
      "=====================================\n",
      "[178/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 114 and best_valid_mse = 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.018873009703094724 RMSE: 0.13737907301730756 R2: 0.9164564581655994 MAE: 0.10931320941769745\n",
      "=====================================\n",
      "[179/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_valid_mse = 0.01522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01438480177998653 RMSE: 0.11993665736540489 R2: 0.9363240252513193 MAE: 0.09485589032834447\n",
      "=====================================\n",
      "[180/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 192 and best_valid_mse = 0.01178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011272783634574114 RMSE: 0.1061733659378571 R2: 0.9500997304626637 MAE: 0.08303623739349428\n",
      "=====================================\n",
      "[181/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 68 and best_valid_mse = 0.04219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04001425817236186 RMSE: 0.2000356422549788 R2: 0.8228722973078819 MAE: 0.1556033037064993\n",
      "=====================================\n",
      "[182/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.02287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02394955416735661 RMSE: 0.15475643497882927 R2: 0.8939845519091895 MAE: 0.12153345741066031\n",
      "=====================================\n",
      "[183/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 144 and best_valid_mse = 0.01834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.020522259605344106 RMSE: 0.14325592345639362 R2: 0.9091558643349631 MAE: 0.11200275627046535\n",
      "=====================================\n",
      "[184/216] START => batchsize: 2048 width: 16 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 164 with best_epoch = 144 and best_valid_mse = 0.01834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.020522259605344106 RMSE: 0.14325592345639362 R2: 0.9091558643349631 MAE: 0.11200275627046535\n",
      "=====================================\n",
      "[185/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.06236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.060704898817594555 RMSE: 0.24638364153813977 R2: 0.7312828036596005 MAE: 0.19739233700311906\n",
      "=====================================\n",
      "[186/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03220998303438931 RMSE: 0.1794713989313877 R2: 0.8574188162115135 MAE: 0.13861582066156267\n",
      "=====================================\n",
      "[187/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 131 and best_valid_mse = 0.02993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.030815414967625476 RMSE: 0.17554319971911608 R2: 0.8635920316900971 MAE: 0.13703465911625412\n",
      "=====================================\n",
      "[188/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 151 with best_epoch = 131 and best_valid_mse = 0.02993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.030815414967625476 RMSE: 0.17554319971911608 R2: 0.8635920316900971 MAE: 0.13703465911625412\n",
      "=====================================\n",
      "[189/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.06877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.06888586448624384 RMSE: 0.26246116757768917 R2: 0.6950688209225215 MAE: 0.21369861745517033\n",
      "=====================================\n",
      "[190/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04208281509685664 RMSE: 0.20514096396589504 R2: 0.8137155928565494 MAE: 0.15882896516635897\n",
      "=====================================\n",
      "[191/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 149 and best_valid_mse = 0.03152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.034285956967484814 RMSE: 0.1851646752690286 R2: 0.8482292795210167 MAE: 0.14689632406781913\n",
      "=====================================\n",
      "[192/216] START => batchsize: 2048 width: 16 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 208 and best_valid_mse = 0.02638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.027387453835023865 RMSE: 0.16549155215606587 R2: 0.8787662947670266 MAE: 0.129957268591092\n",
      "=====================================\n",
      "[193/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.02711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0266129574940405 RMSE: 0.16313478321326969 R2: 0.8821946916407337 MAE: 0.12973152268356963\n",
      "=====================================\n",
      "[194/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 110 and best_valid_mse = 0.01119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01109871858551169 RMSE: 0.10535045602896881 R2: 0.9508702493643665 MAE: 0.08244154306875834\n",
      "=====================================\n",
      "[195/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 138 and best_valid_mse = 0.01021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011520020366622684 RMSE: 0.10733135779734962 R2: 0.9490053087147899 MAE: 0.08290066815547784\n",
      "=====================================\n",
      "[196/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 138 and best_valid_mse = 0.01021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.011520020366622684 RMSE: 0.10733135779734962 R2: 0.9490053087147899 MAE: 0.08290066815547784\n",
      "=====================================\n",
      "[197/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.04177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.042161427337623875 RMSE: 0.2053324799870295 R2: 0.8133676067574295 MAE: 0.1634448310550407\n",
      "=====================================\n",
      "[198/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 109 and best_valid_mse = 0.02202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.022901083125030555 RMSE: 0.1513310382077337 R2: 0.8986257292181993 MAE: 0.11740154129105664\n",
      "=====================================\n",
      "[199/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 136 and best_valid_mse = 0.01978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02018218581748281 RMSE: 0.14206402013698896 R2: 0.9106612399570777 MAE: 0.10926226773160413\n",
      "=====================================\n",
      "[200/216] START => batchsize: 2048 width: 32 steps: 3 learning_rate: 0.01 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 172 with best_epoch = 152 and best_valid_mse = 0.01875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.018940890732262593 RMSE: 0.13762590865190533 R2: 0.9161559750052946 MAE: 0.10690252263606626\n",
      "=====================================\n",
      "[201/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.05197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.05267415633914863 RMSE: 0.2295085103850152 R2: 0.7668318062174347 MAE: 0.18213220431711571\n",
      "=====================================\n",
      "[202/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 115 and best_valid_mse = 0.02153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02218775425949045 RMSE: 0.14895554457451543 R2: 0.9017833612470836 MAE: 0.1133989031515367\n",
      "=====================================\n",
      "[203/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 147 and best_valid_mse = 0.01508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.015228189299080762 RMSE: 0.12340254980785755 R2: 0.9325906736771659 MAE: 0.09403953958296495\n",
      "=====================================\n",
      "[204/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.02 max_epochs: 210\n",
      "\n",
      "Early stopping occurred at epoch 172 with best_epoch = 152 and best_valid_mse = 0.01343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.014174767631636324 RMSE: 0.11905783313850594 R2: 0.9372537654960069 MAE: 0.09067383464346154\n",
      "=====================================\n",
      "[205/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.04883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.04980301838625773 RMSE: 0.22316589879786233 R2: 0.779541227632099 MAE: 0.17210115245975585\n",
      "=====================================\n",
      "[206/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 119 and best_valid_mse = 0.02496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02582528233206318 RMSE: 0.16070246523330992 R2: 0.8856814260769366 MAE: 0.12088472373215618\n",
      "=====================================\n",
      "[207/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_valid_mse = 0.02168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.021193005085485852 RMSE: 0.1455781751688276 R2: 0.906186732545068 MAE: 0.1113527091492405\n",
      "=====================================\n",
      "[208/216] START => batchsize: 2048 width: 32 steps: 5 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 193 and best_valid_mse = 0.01761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.01773025760315883 RMSE: 0.13315501343606567 R2: 0.9215149813884052 MAE: 0.10125293230853606\n",
      "=====================================\n",
      "[209/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 69 and best_valid_mse = 0.04179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.042897338484224064 RMSE: 0.20711672671279852 R2: 0.8101100116716646 MAE: 0.1613298353757505\n",
      "=====================================\n",
      "[210/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 116 and best_valid_mse = 0.02219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.022289862787884637 RMSE: 0.14929789947579516 R2: 0.9013313661361951 MAE: 0.11462825945411793\n",
      "=====================================\n",
      "[211/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 148 and best_valid_mse = 0.01829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.019660786855965687 RMSE: 0.14021692785097556 R2: 0.9129692722550093 MAE: 0.10743393621134506\n",
      "=====================================\n",
      "[212/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.02 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 206 and best_valid_mse = 0.01301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013225320419831845 RMSE: 0.11500139312126548 R2: 0.941456602463019 MAE: 0.08953873845666203\n",
      "=====================================\n",
      "[213/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 70\n",
      "Stop training because you reached max_epochs = 70 with best_epoch = 67 and best_valid_mse = 0.05445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.05180039237641375 RMSE: 0.22759699553468132 R2: 0.7706996226029773 MAE: 0.1748515176603173\n",
      "=====================================\n",
      "[214/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 120\n",
      "Stop training because you reached max_epochs = 120 with best_epoch = 118 and best_valid_mse = 0.03335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.032323093798603326 RMSE: 0.17978624474248114 R2: 0.8569181184420165 MAE: 0.1391890045532711\n",
      "=====================================\n",
      "[215/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 150\n",
      "Stop training because you reached max_epochs = 150 with best_epoch = 146 and best_valid_mse = 0.02844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.028695880851497976 RMSE: 0.1693985857423195 R2: 0.8729743925263288 MAE: 0.13185349519230302\n",
      "=====================================\n",
      "[216/216] START => batchsize: 2048 width: 32 steps: 7 learning_rate: 0.01 max_epochs: 210\n",
      "Stop training because you reached max_epochs = 210 with best_epoch = 196 and best_valid_mse = 0.02115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.02267201920743164 RMSE: 0.15057230557918558 R2: 0.8996397069188274 MAE: 0.11698279194088934\n",
      "=====================================\n",
      "Best model SCORE: 0.9707297378665768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniele\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006612334260205903 RMSE: 0.08131626073674258 R2: 0.9707297440852812 MAE: 0.06346850776455426\n",
      "=====================================\n",
      "(0.9707297440852812, TabNetRegressor(n_d=32, n_a=32, n_steps=5, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1, n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.001, seed=42, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.01}, scheduler_fn=None, scheduler_params={}, mask_type='sparsemax', input_dim=1129, output_dim=1, device_name='auto', n_shared_decoder=1, n_indep_decoder=1))\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "history = None\n",
    "\n",
    "batchsize = [512,1024,2048]\n",
    "width = [8,16,32]\n",
    "steps = [3,5,7]\n",
    "learning_rate = [2e-2,1e-2]\n",
    "max_epochs = [70,120,150,210]\n",
    "\n",
    "best_model_params = None \n",
    "best_r2 = 0\n",
    "\n",
    "total_iterations = len(batchsize) * len(width) * len(steps) * len(learning_rate) * len(max_epochs)\n",
    "current_iteration = 0\n",
    "\n",
    "for batchsize,width,steps,learning_rate,max_epochs in itertools.product(batchsize,width,steps,learning_rate,max_epochs):\n",
    "    current_iteration += 1\n",
    "    print(f\"[{current_iteration}/{total_iterations}] START => batchsize: {batchsize} width: {width} steps: {steps} learning_rate: {learning_rate} max_epochs: {max_epochs}\")\n",
    "    log_name = f'TabNet_{batchsize}_{width}_{steps}_{learning_rate}_{max_epochs}'\n",
    "\n",
    "\n",
    "    writer = SummaryWriter('run/'+log_name)\n",
    "    model = TabNet(ratings, genome_scores,width_values = width, steps = steps, learning_rate = learning_rate)\n",
    "    history = model.train(max_epochs = max_epochs,batchsize = batchsize)\n",
    "    r2score, instance = model.test()\n",
    "    writer.add_hparams({'batchsize': batchsize, 'hidden_size1': width, 'hidden_size2': steps, 'lr': learning_rate, 'num_epochs': max_epochs}, {'metrics/r2': r2score})\n",
    "    \n",
    "    if r2score > best_r2:\n",
    "        best_r2 = r2score\n",
    "        best_model_params = f'{batchsize}_{width}_{steps}_{learning_rate}_{max_epochs}'\n",
    "        model.save(\"model\",best_model_params)\n",
    "        print(f\"New best model: {best_model_params} with r2: {best_r2}\")\n",
    "\n",
    "    writer.flush()\n",
    "writer.close()\n",
    "model = TabNet(ratings,genome_scores)\n",
    "print(f'Best model SCORE: {best_r2}')\n",
    "model.load(f\"model/{best_model_params}.pt.zip\")\n",
    "print(model.test())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['valid_mse'],label='validation')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8bec9e9efeda165029d081619199fa95bb45532afd075062965979786e87119"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
