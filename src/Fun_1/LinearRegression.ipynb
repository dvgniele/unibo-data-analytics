{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 1038893\n",
    "\n",
    "fix_random(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "root = \"../../data/ml-25m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_csv(file: str, nrows=None):\n",
    "    if nrows:\n",
    "        df = pd.read_csv(f\"{root}/{file}\", nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{root}/{file}\")\n",
    "    print(f\"Loaded ml-25m data: {root}/{file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_movies_from_csv(nrows=None):\n",
    "    path = \"movies.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "    print(data.head())\n",
    "    return data\n",
    "\n",
    "def get_ratings_from_csv(nrows=None):\n",
    "    path = \"ratings.csv\"\n",
    "\n",
    "    data = get_data_from_csv(path, nrows)\n",
    "    # data = get_data_from_csv(path)\n",
    "\n",
    "    data.drop(\"timestamp\", axis=1, inplace=True)\n",
    "    # todo: drop user id\n",
    "    #data.drop(\"userId\", axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_tag_relevances_from_csv(nrows=None):\n",
    "    path = \"genome-scores.csv\"\n",
    "\n",
    "    return get_data_from_csv(path, nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores_path = \"genome-scores.csv\"\n",
    "ratings_path = \"ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/ratings.csv\n",
      "Loaded ml-25m data: ../../data/ml-25m/movies.csv\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "          userId  movieId  rating\n",
      "0              1      296     5.0\n",
      "1              1      306     3.5\n",
      "2              1      307     5.0\n",
      "3              1      665     5.0\n",
      "4              1      899     3.5\n",
      "...          ...      ...     ...\n",
      "25000090  162541    50872     4.5\n",
      "25000091  162541    55768     2.5\n",
      "25000092  162541    56176     2.0\n",
      "25000093  162541    58559     4.0\n",
      "25000094  162541    63876     5.0\n",
      "\n",
      "[25000095 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ratings = get_ratings_from_csv()\n",
    "movies = get_movies_from_csv()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ml-25m data: ../../data/ml-25m/genome-scores.csv\n",
      "          movieId  tagId  relevance\n",
      "0               1      1    0.02875\n",
      "1               1      2    0.02375\n",
      "2               1      3    0.06250\n",
      "3               1      4    0.07575\n",
      "4               1      5    0.14075\n",
      "...           ...    ...        ...\n",
      "15584443   206499   1124    0.11000\n",
      "15584444   206499   1125    0.04850\n",
      "15584445   206499   1126    0.01325\n",
      "15584446   206499   1127    0.14025\n",
      "15584447   206499   1128    0.03350\n",
      "\n",
      "[15584448 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "genome_scores = get_tag_relevances_from_csv()\n",
    "print(genome_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARE TEST CON AVG, STD_DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addColumnOperation(ratings,X):\n",
    "     # Compute the mean rating for each user\n",
    "     count_rating = ratings.groupby('movieId', as_index=False)['rating'].count()\n",
    "     std= ratings.groupby('movieId', as_index=False)['rating'].std()\n",
    "     std.fillna(0, inplace=True)\n",
    "     min_ratings= ratings.groupby('movieId', as_index=False)['rating'].min()\n",
    "     max_ratings= ratings.groupby('movieId', as_index=False)['rating'].max()\n",
    "     median= ratings.groupby('movieId', as_index=False)['rating'].median()\n",
    "     operation = pd.DataFrame({'movieId':count_rating['movieId'],'count_rating': count_rating['rating'], 'std': std['rating'], 'min': min_ratings['rating'], 'max': max_ratings['rating'], 'median': median['rating']}) \n",
    "     X = pd.merge(X, operation, on='movieId')\n",
    "     return X\n",
    "    \n",
    "def preprocessing1(relevance, ratings):\n",
    "     # Reduce genome-score size\n",
    "     # Pivot the relevance DataFrame to create a matrix of tag relevance scores for each movie\n",
    "     relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "     counting = relevance.groupby('movieId', as_index=False)['relevance'].count()\n",
    "     print(counting)\n",
    "     # Merge the ratings and relevance data\n",
    "     mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "     X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "     #X = addColumnOperation(ratings,X)\n",
    "     X.columns = X.columns.astype(str)\n",
    "     ratings = None  \n",
    "     # mescolare le righe del DataFrame\n",
    "     #X = X.sample(frac=1).reset_index(drop=True)\n",
    "     \n",
    "     y = X['rating']\n",
    "     X.drop(\"movieId\", axis=1, inplace=True)\n",
    "     X = X.drop('rating', axis=1)\n",
    "     return X,y\n",
    "\n",
    "def preprocessing2(movies,relevance, ratings):\n",
    "     # Merge the ratings and movies data\n",
    "     #X = ratings.merge(movies, on='movieId')\n",
    "     movies_genres = movies['genres'].str.split( '|')\n",
    "     m = pd.get_dummies(movies_genres.apply(pd.Series).stack()).sum(level=0)\n",
    "     movies_genres = pd.concat([movies['movieId'], m], axis=1)\n",
    "\n",
    "     relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "     mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "     \n",
    "     X = mean_ratings.merge(movies_genres, on='movieId')\n",
    "     X = X.merge(relevance_matrix, on='movieId')\n",
    "     X.columns = X.columns.astype(str)\n",
    "     ratings = None  \n",
    "     # mescolare le righe del DataFrame\n",
    "     #X = X.sample(frac=1).reset_index(drop=True)\n",
    "     \n",
    "     y = X['rating']\n",
    "     X.drop(\"movieId\", axis=1, inplace=True)\n",
    "     X = X.drop('rating', axis=1)\n",
    "     \n",
    "     # Apply PCA if n_components is specified\n",
    "     pca = PCA()\n",
    "     X = pca.fit_transform(X)\n",
    "     return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel:\n",
    "    def __init__(self, ratings, relevance,movies, seed=SEED):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.preprocessing import StandardScaler,scale\n",
    "        from sklearn.pipeline import Pipeline\n",
    "\n",
    "        relevance_matrix = relevance.pivot_table(index='movieId', columns='tagId', values='relevance', fill_value=0)\n",
    "        print(relevance_matrix.shape)\n",
    "        #Create One Hot Encoding for genres\n",
    "        movies_genres = movies['genres'].str.split( '|')\n",
    "        m = pd.get_dummies(movies_genres.apply(pd.Series).stack()).sum(level=0)\n",
    "        movies_genres = pd.concat([movies['movieId'], m], axis=1)\n",
    "\n",
    "        # Merge the ratings and relevance data\n",
    "        mean_ratings = ratings.groupby('movieId', as_index=False)['rating'].mean()\n",
    "        X = mean_ratings.merge(relevance_matrix, on='movieId')\n",
    "        X = X.merge(movies_genres, on='movieId')\n",
    "    \n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        ratings = None  \n",
    "\n",
    "        # mescolare le righe del DataFrame\n",
    "        #X = X.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        X.drop(\"movieId\", axis=1, inplace=True)\n",
    "        self.y = X['rating']\n",
    "        self.X = X.drop('rating', axis=1)\n",
    "        \n",
    "        # Apply PCA if n_components is specified\n",
    "       \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=seed)\n",
    "        print(self.X_train.shape, self.X_test.shape, self.y_train.shape, self.y_test.shape)\n",
    "\n",
    "        self.pipelineScaled = Pipeline([('Standard Scaler',StandardScaler()),('pca',PCA(n_components=0.95)),('Linear Regression',LinearRegression())],verbose=True)\n",
    "        self.pipeline = Pipeline([('pca',PCA(n_components=0.95)),('Linear Regression',LinearRegression())],verbose=True)\n",
    "        self.onlinereg = Pipeline([('Linear Regression',LinearRegression())],verbose=True)\n",
    "       \n",
    "       #self.model = LinearRegression()\n",
    "        '''pca = PCA()\n",
    "        pca.fit(self.X_train)\n",
    "        self.X_train_t = pca.transform(self.X_train)\n",
    "        self.X_test_t = pca.transform(self.X_test)\n",
    "        print(self.X_train)\n",
    "        print(self.X_train_t)\n",
    "        plt.scatter(self.X_train_t[:, 0], self.X_train_t[:, 1], c=self.y_train, cmap='viridis')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.scatter(self.X_test_t[:, 0], self.X_test_t[:, 1], c=self.y_test, cmap='viridis')\n",
    "        plt.show()\n",
    "        self.model = LinearRegression()\n",
    "        self.params = {\n",
    "                'fit_intercept': [True, False],\n",
    "                'normalize': [True, False],\n",
    "                'copy_X': [True, False]\n",
    "            }\n",
    "            '''\n",
    "       \n",
    "            \n",
    "    def train(self):\n",
    "        for model in [self.pipelineScaled,self.pipeline,self.onlinereg]:\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "    \n",
    "        \n",
    "     # Hyperparameter tuning\n",
    "    def tuning(self,X,y):\n",
    "        grid_search = GridSearchCV(self.model, param_grid=self.params, cv=3, scoring='neg_mean_squared_error', verbose=10, n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        self.model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Re-initialize the model with the best hyperparameters\n",
    "        #self.model = LinearRegression(**grid_search.best_params_)\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        #model = self.model\n",
    "        # Predict ratings for the test data\n",
    "        for model in [self.pipelineScaled, self.pipeline, self.onlinereg]:\n",
    "            y_pred = model.predict(self.X_test)\n",
    "\n",
    "            # Compute the mean squared error\n",
    "            mse = mean_squared_error(self.y_test, y_pred)\n",
    "            rmse = mean_squared_error(self.y_test, y_pred, squared=False)\n",
    "            r2 = r2_score(self.y_test, y_pred)\n",
    "            mae = mean_absolute_error(self.y_test, y_pred)\n",
    "\n",
    "            print(f\"MSE: {mse} RMSE: {rmse} R2: {r2} MAE: {mae}\")\n",
    "            score = model.score(self.X_test, self.y_test)\n",
    "            print(f\"Score:{score}\")\n",
    "            print(\"=====================================\")\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13816, 1128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\AppData\\Local\\Temp\\ipykernel_18016\\1116060927.py:11: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  m = pd.get_dummies(movies_genres.apply(pd.Series).stack()).sum(level=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11052, 1148) (2764, 1148) (11052,) (2764,)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegressionModel(ratings,genome_scores,movies, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ... (step 1 of 3) Processing Standard Scaler, total=   0.7s\n",
      "[Pipeline] ............... (step 2 of 3) Processing pca, total=   8.9s\n",
      "[Pipeline] . (step 3 of 3) Processing Linear Regression, total=   2.9s\n",
      "[Pipeline] ............... (step 1 of 2) Processing pca, total=   9.1s\n",
      "[Pipeline] . (step 2 of 2) Processing Linear Regression, total=   2.3s\n",
      "[Pipeline] . (step 1 of 1) Processing Linear Regression, total=   5.7s\n"
     ]
    }
   ],
   "source": [
    "lr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006597745042071569 RMSE: 0.08122650455406516 R2: 0.9713379618311104 MAE: 0.0633443088299459\n",
      "Score:0.9713379618311104\n",
      "=====================================\n",
      "MSE: 0.006559006068277205 RMSE: 0.08098769084420919 R2: 0.9715062523513468 MAE: 0.06231471983066634\n",
      "Score:0.9715062523513468\n",
      "=====================================\n",
      "MSE: 0.005221310694310264 RMSE: 0.0722586375066003 R2: 0.977317491740335 MAE: 0.05602101705311702\n",
      "Score:0.977317491740335\n",
      "=====================================\n",
      "MSE: 0.005221310694310264 RMSE: 0.0722586375066003 R2: 0.977317491740335 MAE: 0.05602101705311702\n",
      "Score:0.977317491740335\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "lr.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a32af23fa2dfa3c5d159e60107838eb3f09fc5f820c64a56f9f0f73009b4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
